{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Modal Autoencoder for Image-Caption Retrieval\n",
    "\n",
    "This notebook implements a cross-modal autoencoder where:\n",
    "- Image latents decode into text feature space\n",
    "- Text latents decode into image feature space\n",
    "\n",
    "This forces the shared latent space to capture cross-modal information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Pre-extracted Features\n",
    "\n",
    "We'll use the features already extracted from the previous notebook:\n",
    "- Image features: ResNet50 (2048-dim)\n",
    "- Caption features: BERT (768-dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes: (5663, 2048) (28315, 768) (28315,)\n",
      "Val shapes: (1214, 2048) (6070, 768) (6070,)\n",
      "Test shapes: (1214, 2048) (6070, 768) (6070,)\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-split data\n",
    "image_train = np.load(\"train_image_features.npy\")\n",
    "caption_train = np.load(\"train_caption_features.npy\")\n",
    "cap2img_train = np.load(\"train_caption_to_image.npy\")\n",
    "\n",
    "image_val = np.load(\"val_image_features.npy\")\n",
    "caption_val = np.load(\"val_caption_features.npy\")\n",
    "cap2img_val = np.load(\"val_caption_to_image.npy\")\n",
    "\n",
    "image_test = np.load(\"test_image_features.npy\")\n",
    "caption_test = np.load(\"test_caption_features.npy\")\n",
    "cap2img_test = np.load(\"test_caption_to_image.npy\")\n",
    "\n",
    "print(\"Train shapes:\", image_train.shape, caption_train.shape, cap2img_train.shape)\n",
    "print(\"Val shapes:\", image_val.shape, caption_val.shape, cap2img_val.shape)\n",
    "print(\"Test shapes:\", image_test.shape, caption_test.shape, cap2img_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normalize Features\n",
    "\n",
    "Normalize using training set statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization complete!\n"
     ]
    }
   ],
   "source": [
    "# Compute normalization parameters from training set\n",
    "img_mean = image_train.mean(axis=0, keepdims=True)\n",
    "img_std = image_train.std(axis=0, keepdims=True) + 1e-6\n",
    "\n",
    "txt_mean = caption_train.mean(axis=0, keepdims=True)\n",
    "txt_std = caption_train.std(axis=0, keepdims=True) + 1e-6\n",
    "\n",
    "# Apply normalization\n",
    "def normalize_images(x):\n",
    "    return (x - img_mean) / img_std\n",
    "\n",
    "def normalize_texts(x):\n",
    "    return (x - txt_mean) / txt_std\n",
    "\n",
    "image_train = normalize_images(image_train)\n",
    "image_val = normalize_images(image_val)\n",
    "image_test = normalize_images(image_test)\n",
    "\n",
    "caption_train = normalize_texts(caption_train)\n",
    "caption_val = normalize_texts(caption_val)\n",
    "caption_test = normalize_texts(caption_test)\n",
    "\n",
    "print(\"Normalization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset and DataLoader\n",
    "\n",
    "Pairs captions with their corresponding images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionImagePairedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Iterates over captions. For index i, returns:\n",
    "      caption_features[i], image_features[caption_to_image_idx[i]]\n",
    "    \"\"\"\n",
    "    def __init__(self, caption_feats, image_feats, caption_to_image_idx):\n",
    "        assert len(caption_feats) == len(caption_to_image_idx)\n",
    "        self.caption_feats = caption_feats.astype(np.float32)\n",
    "        self.image_feats = image_feats.astype(np.float32)\n",
    "        self.cap2img = caption_to_image_idx.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.caption_feats)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cap = self.caption_feats[idx]\n",
    "        img = self.image_feats[self.cap2img[idx]]\n",
    "        return {\"image\": torch.from_numpy(img), \"caption\": torch.from_numpy(cap)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"latent_dim\": 512,\n",
    "    \"img_input_dim\": 2048,\n",
    "    \"txt_input_dim\": 768,\n",
    "    \"img_encoder_hidden\": 1024,\n",
    "    \"txt_encoder_hidden\": 512,\n",
    "    \"batch_size\": 128,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"epochs\": 40,\n",
    "    \"lambda_recon\": 1.0,      # Weight for cross-modal reconstruction\n",
    "    \"lambda_contrastive\": 1.0,  # Weight for contrastive loss\n",
    "    \"temperature\": 0.07,       # Temperature for contrastive loss\n",
    "    \"checkpoint_dir\": \"./cross_modal_checkpoints\",\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(config[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Modal Autoencoder Architecture\n",
    "\n",
    "Key idea:\n",
    "- Image encoder → latent → Text decoder (outputs text features)\n",
    "- Text encoder → latent → Image decoder (outputs image features)\n",
    "\n",
    "Both encoders output the same latent dimension, forcing shared representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture defined!\n"
     ]
    }
   ],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"Encodes images (2048-dim) to shared latent space (512-dim)\"\"\"\n",
    "    def __init__(self, input_dim=2048, hidden_dim=1024, latent_dim=512):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Encodes text (768-dim) to shared latent space (512-dim)\"\"\"\n",
    "    def __init__(self, input_dim=768, hidden_dim=512, latent_dim=512):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class ImageDecoder(nn.Module):\n",
    "    \"\"\"Decodes from latent space (512-dim) to IMAGE features (2048-dim)\"\"\"\n",
    "    def __init__(self, latent_dim=512, hidden_dim=1024, output_dim=2048):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "\n",
    "class TextDecoder(nn.Module):\n",
    "    \"\"\"Decodes from latent space (512-dim) to TEXT features (768-dim)\"\"\"\n",
    "    def __init__(self, latent_dim=512, hidden_dim=512, output_dim=768):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "\n",
    "print(\"Model architecture defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss functions defined!\n"
     ]
    }
   ],
   "source": [
    "def contrastive_loss(z_img, z_txt, temperature=0.07):\n",
    "    \"\"\"\n",
    "    Contrastive loss to align image and text embeddings.\n",
    "    Encourages matching pairs to be close and non-matching pairs to be far.\n",
    "    \"\"\"\n",
    "    # Normalize embeddings\n",
    "    z_img_norm = F.normalize(z_img, dim=1)\n",
    "    z_txt_norm = F.normalize(z_txt, dim=1)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    logits = torch.matmul(z_img_norm, z_txt_norm.T) / temperature\n",
    "    \n",
    "    # Labels: diagonal elements are positive pairs\n",
    "    labels = torch.arange(z_img.size(0)).to(z_img.device)\n",
    "    \n",
    "    # Cross-entropy in both directions\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "    \n",
    "    return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "\n",
    "# Reconstruction loss\n",
    "recon_loss_fn = nn.MSELoss()\n",
    "\n",
    "print(\"Loss functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Models and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sfowler14/miniconda3/envs/DLD/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models initialized with 6,560,512 total parameters\n"
     ]
    }
   ],
   "source": [
    "# Instantiate models\n",
    "img_encoder = ImageEncoder(\n",
    "    input_dim=config[\"img_input_dim\"],\n",
    "    hidden_dim=config[\"img_encoder_hidden\"],\n",
    "    latent_dim=config[\"latent_dim\"]\n",
    ").to(device)\n",
    "\n",
    "txt_encoder = TextEncoder(\n",
    "    input_dim=config[\"txt_input_dim\"],\n",
    "    hidden_dim=config[\"txt_encoder_hidden\"],\n",
    "    latent_dim=config[\"latent_dim\"]\n",
    ").to(device)\n",
    "\n",
    "img_decoder = ImageDecoder(\n",
    "    latent_dim=config[\"latent_dim\"],\n",
    "    hidden_dim=config[\"img_encoder_hidden\"],\n",
    "    output_dim=config[\"img_input_dim\"]\n",
    ").to(device)\n",
    "\n",
    "txt_decoder = TextDecoder(\n",
    "    latent_dim=config[\"latent_dim\"],\n",
    "    hidden_dim=config[\"txt_encoder_hidden\"],\n",
    "    output_dim=config[\"txt_input_dim\"]\n",
    ").to(device)\n",
    "\n",
    "# Optimizer for all parameters\n",
    "params = (\n",
    "    list(img_encoder.parameters()) + \n",
    "    list(txt_encoder.parameters()) + \n",
    "    list(img_decoder.parameters()) + \n",
    "    list(txt_decoder.parameters())\n",
    ")\n",
    "optimizer = Adam(params, lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "print(f\"Models initialized with {sum(p.numel() for p in params):,} total parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 222, Val batches: 48\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CaptionImagePairedDataset(caption_train, image_train, cap2img_train)\n",
    "val_dataset = CaptionImagePairedDataset(caption_val, image_val, cap2img_val)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config[\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config[\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop\n",
    "\n",
    "Cross-modal reconstruction:\n",
    "- Image → img_encoder → z_img → txt_decoder → reconstructed text features\n",
    "- Text → txt_encoder → z_txt → img_decoder → reconstructed image features\n",
    "\n",
    "Plus contrastive loss to align z_img and z_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader, training=True):\n",
    "    if training:\n",
    "        img_encoder.train()\n",
    "        txt_encoder.train()\n",
    "        img_decoder.train()\n",
    "        txt_decoder.train()\n",
    "    else:\n",
    "        img_encoder.eval()\n",
    "        txt_encoder.eval()\n",
    "        img_decoder.eval()\n",
    "        txt_decoder.eval()\n",
    "\n",
    "    total_recon_i2t = 0.0  # Image to text reconstruction\n",
    "    total_recon_t2i = 0.0  # Text to image reconstruction\n",
    "    total_contrast = 0.0   # Contrastive loss\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"train\" if training else \"val\")\n",
    "    with torch.set_grad_enabled(training):\n",
    "        for batch in pbar:\n",
    "            imgs = batch[\"image\"].to(device)    # (B, 2048)\n",
    "            caps = batch[\"caption\"].to(device)  # (B, 768)\n",
    "            batch_size = imgs.shape[0]\n",
    "\n",
    "            # Forward pass: CROSS-MODAL reconstruction\n",
    "            z_img = img_encoder(imgs)              # Image → latent\n",
    "            z_txt = txt_encoder(caps)              # Text → latent\n",
    "            \n",
    "            txt_recon = txt_decoder(z_img)         # Image latent → text features\n",
    "            img_recon = img_decoder(z_txt)         # Text latent → image features\n",
    "\n",
    "            # Losses\n",
    "            L_i2t = recon_loss_fn(txt_recon, caps)  # Image should reconstruct text\n",
    "            L_t2i = recon_loss_fn(img_recon, imgs)  # Text should reconstruct image\n",
    "            L_contrast = contrastive_loss(z_img, z_txt, temperature=config[\"temperature\"])\n",
    "\n",
    "            # Total loss\n",
    "            loss = (\n",
    "                config[\"lambda_recon\"] * (L_i2t + L_t2i) + \n",
    "                config[\"lambda_contrastive\"] * L_contrast\n",
    "            )\n",
    "\n",
    "            if training:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Accumulate metrics\n",
    "            total_recon_i2t += L_i2t.item() * batch_size\n",
    "            total_recon_t2i += L_t2i.item() * batch_size\n",
    "            total_contrast += L_contrast.item() * batch_size\n",
    "            total_loss += loss.item() * batch_size\n",
    "            n_samples += batch_size\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{total_loss / n_samples:.4f}\",\n",
    "                \"i2t\": f\"{total_recon_i2t / n_samples:.4f}\",\n",
    "                \"t2i\": f\"{total_recon_t2i / n_samples:.4f}\",\n",
    "                \"contrast\": f\"{total_contrast / n_samples:.4f}\"\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / n_samples,\n",
    "        \"L_i2t\": total_recon_i2t / n_samples,\n",
    "        \"L_t2i\": total_recon_t2i / n_samples,\n",
    "        \"L_contrast\": total_contrast / n_samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, config[\"epochs\"] + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{config['epochs']} ===\")\n",
    "    train_metrics = run_epoch(train_loader, training=True)\n",
    "    val_metrics = run_epoch(val_loader, training=False)\n",
    "\n",
    "    print(f\"Train loss: {train_metrics['loss']:.4f} | Val loss: {val_metrics['loss']:.4f}\")\n",
    "\n",
    "    # Save checkpoint every epoch\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch,\n",
    "        \"img_encoder\": img_encoder.state_dict(),\n",
    "        \"txt_encoder\": txt_encoder.state_dict(),\n",
    "        \"img_decoder\": img_decoder.state_dict(),\n",
    "        \"txt_decoder\": txt_decoder.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"train_metrics\": train_metrics,\n",
    "        \"val_metrics\": val_metrics,\n",
    "        \"config\": config\n",
    "    }\n",
    "    ckpt_path = os.path.join(config[\"checkpoint_dir\"], f\"cross_modal_epoch{epoch}.pt\")\n",
    "    torch.save(ckpt, ckpt_path)\n",
    "\n",
    "    # Save best checkpoint\n",
    "    if val_metrics[\"loss\"] < best_val_loss:\n",
    "        best_val_loss = val_metrics[\"loss\"]\n",
    "        torch.save(ckpt, os.path.join(config[\"checkpoint_dir\"], \"cross_modal_best.pt\"))\n",
    "        print(\"Saved best checkpoint.\")\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluation: Load Best Model and Compute Retrieval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best checkpoint from epoch 12\n"
     ]
    }
   ],
   "source": [
    "# Load best checkpoint\n",
    "best_ckpt_path = os.path.join(config[\"checkpoint_dir\"], \"cross_modal_best.pt\")\n",
    "ckpt = torch.load(best_ckpt_path, map_location=device)\n",
    "\n",
    "img_encoder.load_state_dict(ckpt[\"img_encoder\"])\n",
    "txt_encoder.load_state_dict(ckpt[\"txt_encoder\"])\n",
    "img_encoder.eval()\n",
    "txt_encoder.eval()\n",
    "\n",
    "print(f\"Loaded best checkpoint from epoch {ckpt['epoch']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(image_feats, caption_feats):\n",
    "    \"\"\"\n",
    "    Encode images and captions into latent space.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Encode images\n",
    "        Z_imgs = []\n",
    "        for i in range(0, image_feats.shape[0], 256):\n",
    "            batch = torch.from_numpy(image_feats[i:i+256]).float().to(device)\n",
    "            z = img_encoder(batch)\n",
    "            Z_imgs.append(z.cpu().numpy())\n",
    "        Z_imgs = np.concatenate(Z_imgs, axis=0)\n",
    "\n",
    "        # Encode captions\n",
    "        Z_caps = []\n",
    "        for i in range(0, caption_feats.shape[0], 256):\n",
    "            batch = torch.from_numpy(caption_feats[i:i+256]).float().to(device)\n",
    "            z = txt_encoder(batch)\n",
    "            Z_caps.append(z.cpu().numpy())\n",
    "        Z_caps = np.concatenate(Z_caps, axis=0)\n",
    "\n",
    "    return Z_imgs, Z_caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_metrics(Z_caps, Z_imgs, caption_to_image_idx):\n",
    "    \"\"\"\n",
    "    Compute Recall@K metrics for image retrieval given captions.\n",
    "    \"\"\"\n",
    "    sims = cosine_similarity(Z_caps, Z_imgs)  # (num_caps, num_imgs)\n",
    "    ranks = []\n",
    "    for i, true_img_idx in enumerate(caption_to_image_idx):\n",
    "        sim_scores = sims[i]\n",
    "        sorted_indices = np.argsort(-sim_scores)  # descending\n",
    "        rank = np.where(sorted_indices == true_img_idx)[0][0] + 1\n",
    "        ranks.append(rank)\n",
    "\n",
    "    ranks = np.array(ranks)\n",
    "    recall_at_1 = np.mean(ranks <= 1)\n",
    "    recall_at_5 = np.mean(ranks <= 5)\n",
    "    recall_at_10 = np.mean(ranks <= 10)\n",
    "    med_rank = np.median(ranks)\n",
    "\n",
    "    return {\n",
    "        \"Recall@1\": recall_at_1,\n",
    "        \"Recall@5\": recall_at_5,\n",
    "        \"Recall@10\": recall_at_10,\n",
    "        \"MedianRank\": med_rank\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding validation set...\n",
      "Encoded shapes: (1214, 512), (6070, 512)\n",
      "\n",
      "Validation Set Metrics:\n",
      "  Recall@1: 0.1333\n",
      "  Recall@5: 0.3629\n",
      "  Recall@10: 0.4906\n",
      "  MedianRank: 11.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoding validation set...\")\n",
    "Z_imgs_val, Z_caps_val = encode_features(image_val, caption_val)\n",
    "print(f\"Encoded shapes: {Z_imgs_val.shape}, {Z_caps_val.shape}\")\n",
    "\n",
    "metrics_val = retrieval_metrics(Z_caps_val, Z_imgs_val, cap2img_val)\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "for k, v in metrics_val.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test set...\n",
      "Encoded shapes: (1214, 512), (6070, 512)\n",
      "\n",
      "Test Set Metrics:\n",
      "  Recall@1: 0.1336\n",
      "  Recall@5: 0.3718\n",
      "  Recall@10: 0.5033\n",
      "  MedianRank: 10.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoding test set...\")\n",
    "Z_imgs_test, Z_caps_test = encode_features(image_test, caption_test)\n",
    "print(f\"Encoded shapes: {Z_imgs_test.shape}, {Z_caps_test.shape}\")\n",
    "\n",
    "metrics_test = retrieval_metrics(Z_caps_test, Z_imgs_test, cap2img_test)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for k, v in metrics_test.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualization (Optional)\n",
    "\n",
    "Visualize some retrieval results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load additional data for visualization\n",
    "import pandas as pd\n",
    "\n",
    "# Load original dataframe and metadata\n",
    "df = pd.read_csv('./flickr8k_data/captions.txt')\n",
    "image_names = np.load('flickr8k_image_names.npy')\n",
    "\n",
    "# Load split indices to get validation masks\n",
    "from sklearn.model_selection import train_test_split\n",
    "n_images = len(image_names)\n",
    "indices = np.arange(n_images)\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.30, random_state=42, shuffle=True)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# Create masks\n",
    "caption_to_image_idx = df[\"image\"].map({name: i for i, name in enumerate(image_names)}).values.astype(int)\n",
    "val_mask = np.isin(caption_to_image_idx, val_idx)\n",
    "\n",
    "# Get validation caption texts and image names\n",
    "val_caption_texts = df[\"caption\"].values[val_mask]\n",
    "val_image_names = np.array([image_names[i] for i in val_idx])\n",
    "\n",
    "print(f\"Loaded {len(val_caption_texts)} validation captions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"./flickr8k_data/Images\"\n",
    "\n",
    "def show_top_images_for_caption(caption_idx, top_k=5):\n",
    "    \"\"\"\n",
    "    Show top-k retrieved validation images for a given caption index.\n",
    "    \"\"\"\n",
    "    caption_embedding = Z_caps_val[caption_idx].reshape(1, -1)\n",
    "    sims = cosine_similarity(caption_embedding, Z_imgs_val)[0]\n",
    "    top_img_indices = np.argsort(-sims)[:top_k]\n",
    "\n",
    "    print(f\"\\nCAPTION: {val_caption_texts[caption_idx]}\")\n",
    "    true_img_idx = cap2img_val[caption_idx]\n",
    "    print(f\"TRUE IMAGE: {val_image_names[true_img_idx]} (index {true_img_idx})\")\n",
    "    \n",
    "    plt.figure(figsize=(18, 4))\n",
    "    \n",
    "    # Show true image\n",
    "    true_img_name = val_image_names[true_img_idx]\n",
    "    true_img_path = os.path.join(image_dir, true_img_name)\n",
    "    try:\n",
    "        true_img = Image.open(true_img_path)\n",
    "        plt.subplot(1, top_k + 1, 1)\n",
    "        plt.imshow(true_img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"TRUE IMAGE\", fontweight='bold', color='green')\n",
    "    except Exception as e:\n",
    "        print(f\"Could not open true image: {e}\")\n",
    "    \n",
    "    # Show retrieved images\n",
    "    for i, img_idx in enumerate(top_img_indices):\n",
    "        img_name = val_image_names[img_idx]\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not open {img_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        plt.subplot(1, top_k + 1, i + 2)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if img_idx == true_img_idx:\n",
    "            plt.title(f\"Rank {i+1} ✓\", fontweight='bold', color='green')\n",
    "        else:\n",
    "            plt.title(f\"Rank {i+1}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show some random examples\n",
    "print(\"\\n=== Retrieval Examples ===\")\n",
    "for i in random.sample(range(len(Z_caps_val)), 3):\n",
    "    show_top_images_for_caption(i, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
