{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "41d92e9d-41e9-4e9e-ab17-06d210dcb4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84583e74-3167-497b-84a2-05e99c1902e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted to: ./flickr8k_data\n",
      "Contents: ['captions.txt', 'Images']\n"
     ]
    }
   ],
   "source": [
    "# Path to your zip file\n",
    "zip_path = os.path.expanduser('~/Downloads/archive(1).zip')\n",
    "extract_dir = './flickr8k_data'\n",
    "\n",
    "# Extract only if not already done\n",
    "if not os.path.exists(extract_dir):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(\"Extracted to:\", extract_dir)\n",
    "print(\"Contents:\", os.listdir(extract_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c23613b-4b9f-4cb5-8ad7-18de92763088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       image  \\\n",
      "0  1000268201_693b08cb0e.jpg   \n",
      "1  1000268201_693b08cb0e.jpg   \n",
      "2  1000268201_693b08cb0e.jpg   \n",
      "3  1000268201_693b08cb0e.jpg   \n",
      "4  1000268201_693b08cb0e.jpg   \n",
      "\n",
      "                                             caption  \n",
      "0  A child in a pink dress is climbing up a set o...  \n",
      "1              A girl going into a wooden building .  \n",
      "2   A little girl climbing into a wooden playhouse .  \n",
      "3  A little girl climbing the stairs to her playh...  \n",
      "4  A little girl in a pink dress going into a woo...  \n",
      "\n",
      "Number of unique images: 8091\n"
     ]
    }
   ],
   "source": [
    "captions_path = os.path.join(extract_dir, 'captions.txt')\n",
    "df = pd.read_csv(captions_path)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"\\nNumber of unique images: {df['image'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b5f786-9708-48bc-8f4e-74f8a33ce4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Image transform (standard ImageNet normalization) ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Dataset class for images ---\n",
    "class FlickrImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_filenames, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_filenames = list(image_filenames)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_filenames[idx]\n",
    "        path = os.path.join(self.image_dir, img_name)\n",
    "        # wrap in try-except to catch corrupt images\n",
    "        try:\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            # if image fails to open, create a black image instead and log\n",
    "            print(f\"Failed to open {path}: {e}\")\n",
    "            image = Image.new('RGB', (224,224))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4241e3a5-c3cf-4a44-aac5-41b16073cf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12089\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12089\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# --- Model setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*(list(resnet.children())[:-1]))  # remove FC layer\n",
    "resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "# --- Create dataset and dataloader ---\n",
    "image_dir = os.path.join(extract_dir, \"Images\")\n",
    "unique_images = df[\"image\"].unique()\n",
    "image_dataset = FlickrImageDataset(image_dir, unique_images, transform)\n",
    "image_loader = DataLoader(image_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e945bc06-f5ba-4ced-a3c4-d00b25ab4c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12089\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12089\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# test model forward\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*(list(resnet.children())[:-1])).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "085dd006-09e8-4cbe-95de-4243928ca6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames = sorted(os.listdir(image_dir))   # better to use df['image'].unique() if you want same order\n",
    "dataset = FlickrImageDataset(image_dir, image_filenames, transform)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)  # num_workers=0 is safest\n",
    "\n",
    "image_features_list = []\n",
    "image_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2532870d-6a66-4a27-9b0e-240facf3136b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting image features: 100%|██████████| 253/253 [03:10<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. image_features shape: (8091, 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for imgs, names in tqdm(loader, desc=\"Extracting image features\"):\n",
    "        imgs = imgs.to(device)\n",
    "        feats = resnet(imgs)               # (B, 2048, 1, 1)\n",
    "        feats = feats.view(feats.size(0), -1)  # (B, 2048)\n",
    "        image_features_list.append(feats.cpu())\n",
    "        image_names.extend(names)\n",
    "\n",
    "image_features = torch.cat(image_features_list, dim=0).numpy()\n",
    "print(\"Done. image_features shape:\", image_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e8c92bf-4053-411d-8ea1-387e1bc80261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# --- Device and model setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert.to(device)\n",
    "bert.eval()\n",
    "\n",
    "# --- Dataset for captions ---\n",
    "class FlickrCaptionDataset(Dataset):\n",
    "    def __init__(self, captions):\n",
    "        self.captions = captions\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.captions[idx]\n",
    "        # Return plain text; we’ll tokenize in collate_fn for batching\n",
    "        return text\n",
    "\n",
    "# --- Custom collate_fn to batch tokenize ---\n",
    "def collate_fn(batch_texts):\n",
    "    return tokenizer(batch_texts, return_tensors='pt',\n",
    "                     truncation=True, padding=True, max_length=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25b7a232-4bda-431f-b763-24ad6e5145b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create dataset and dataloader ---\n",
    "caption_dataset = FlickrCaptionDataset(df[\"caption\"].tolist())\n",
    "caption_loader = DataLoader(\n",
    "    caption_dataset,\n",
    "    batch_size=32,         \n",
    "    shuffle=False,\n",
    "    num_workers=0,         \n",
    "    collate_fn=collate_fn  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dea1e630-d628-4cef-b566-be056a36f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting caption features: 100%|██████████| 1265/1265 [05:01<00:00,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption features shape: (40455, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Extract features ---\n",
    "caption_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(caption_loader, desc=\"Extracting caption features\"):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = bert(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]  # (B, 768)\n",
    "        caption_features.append(cls_embeddings.cpu())\n",
    "\n",
    "caption_features = torch.cat(caption_features, dim=0).numpy()\n",
    "\n",
    "print(\"Caption features shape:\", caption_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05249b54-abb8-4e47-ba59-c7b20149aa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caption_to_image_idx shape: (40455,)\n",
      "Example mapping: [('A child in a pink dress is climbing up a set of stairs in an entry way .', 0), ('A girl going into a wooden building .', 0), ('A little girl climbing into a wooden playhouse .', 0)]\n"
     ]
    }
   ],
   "source": [
    "# Map image filename to its index\n",
    "image_to_idx = {name: i for i, name in enumerate(image_names)}\n",
    "\n",
    "# For each caption row, find which image it corresponds to\n",
    "caption_to_image_idx = df[\"image\"].map(image_to_idx).values\n",
    "\n",
    "print(\"caption_to_image_idx shape:\", caption_to_image_idx.shape)\n",
    "print(\"Example mapping:\", list(zip(df['caption'][:3], caption_to_image_idx[:3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e74e7d1d-f1ba-40a8-9206-91cb3c47846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"flickr8k_image_features.npy\", image_features)\n",
    "np.save(\"flickr8k_caption_features.npy\", caption_features)\n",
    "np.save(\"flickr8k_caption_to_image.npy\", caption_to_image_idx)\n",
    "np.save(\"flickr8k_image_names.npy\", np.array(image_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61562bc4-f27c-4e4b-b95f-5fca476671b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on images\n",
    "n_images = len(image_names)\n",
    "indices = np.arange(n_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "577132b8-5241-475f-8be3-162740a15160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 5663, Val: 1214, Test: 1214\n"
     ]
    }
   ],
   "source": [
    "# 70/15/15 split\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.30, random_state=42, shuffle=True)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42, shuffle=True)\n",
    "print(f\"Train images: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afdcdbb8-a6fa-4155-8e4f-19c7285e7449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks for captions \n",
    "caption_to_image_idx = caption_to_image_idx.astype(int)\n",
    "\n",
    "train_mask = np.isin(caption_to_image_idx, train_idx)\n",
    "val_mask = np.isin(caption_to_image_idx, val_idx)\n",
    "test_mask = np.isin(caption_to_image_idx, test_idx)\n",
    "\n",
    "# Split image features \n",
    "image_train = image_features[train_idx]\n",
    "image_val   = image_features[val_idx]\n",
    "image_test  = image_features[test_idx]\n",
    "\n",
    "# Split captions (and keep their alignment)\n",
    "caption_train = caption_features[train_mask]\n",
    "caption_val   = caption_features[val_mask]\n",
    "caption_test  = caption_features[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "360512a8-752e-4295-bfad-00b00c4eb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link captions to local image indices within each split\n",
    "def remap_caption_indices(global_indices, split_indices):\n",
    "    \"\"\"\n",
    "    Convert global image indices in caption_to_image_idx to 0..len(split_indices)-1 within that split.\n",
    "    \"\"\"\n",
    "    mapping = {g: i for i, g in enumerate(split_indices)}\n",
    "    return np.array([mapping[i] for i in global_indices if i in mapping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d4cdda6-8225-493f-b5cd-40777cf9b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_to_train_img = remap_caption_indices(caption_to_image_idx[train_mask], train_idx)\n",
    "caption_to_val_img   = remap_caption_indices(caption_to_image_idx[val_mask], val_idx)\n",
    "caption_to_test_img  = remap_caption_indices(caption_to_image_idx[test_mask], test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d3891c16-d50d-455d-bdea-4f6925c451b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split shapes:\n",
      "  Image features: (5663, 2048)\n",
      "  Caption features: (28315, 768)\n",
      "  Caption→Image indices: (28315,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(\"Train split shapes:\")\n",
    "print(\"  Image features:\", image_train.shape)\n",
    "print(\"  Caption features:\", caption_train.shape)\n",
    "print(\"  Caption→Image indices:\", caption_to_train_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "669c7533-fbbb-4224-988c-e07b19952562",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"train_image_features.npy\", image_train)\n",
    "np.save(\"val_image_features.npy\", image_val)\n",
    "np.save(\"test_image_features.npy\", image_test)\n",
    "\n",
    "np.save(\"train_caption_features.npy\", caption_train)\n",
    "np.save(\"val_caption_features.npy\", caption_val)\n",
    "np.save(\"test_caption_features.npy\", caption_test)\n",
    "\n",
    "np.save(\"train_caption_to_image.npy\", caption_to_train_img)\n",
    "np.save(\"val_caption_to_image.npy\", caption_to_val_img)\n",
    "np.save(\"test_caption_to_image.npy\", caption_to_test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a46b34b-ec97-4a65-9cb5-fb0b9ab46bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now time to set up and train the actual multi-modal autoencoder\n",
    "config = {\n",
    "    \"image_feat_path\": \"train_image_features.npy\",   # we'll load proper files below\n",
    "    \"caption_feat_path\": \"train_caption_features.npy\",\n",
    "    \"caption_to_image_path\": \"train_caption_to_image.npy\",\n",
    "    \"val_image_feat_path\": \"val_image_features.npy\",\n",
    "    \"val_caption_feat_path\": \"val_caption_features.npy\",\n",
    "    \"val_caption_to_image_path\": \"val_caption_to_image.npy\",\n",
    "    \"latent_dim\": 512,\n",
    "    \"img_input_dim\": 2048,\n",
    "    \"txt_input_dim\": 768,\n",
    "    \"img_hidden\": 1024,\n",
    "    \"txt_hidden\": 512,\n",
    "    \"batch_size\": 128,    # try 128; lower if memory limited (e.g., 64)\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"epochs\": 40,\n",
    "    \"lambda_align\": 1.0,  # weight for latent alignment loss; tuneable\n",
    "    \"checkpoint_dir\": \"./corr_ae_checkpoints\",\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ae49527-690e-440f-babb-1fa1fe3a18ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "\n",
    "os.makedirs(config[\"checkpoint_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "95c77066-c042-42ad-ba98-966847dd5981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes (train): (5663, 2048) (28315, 768) (28315,)\n",
      "Shapes (val): (1214, 2048) (6070, 768) (6070,)\n"
     ]
    }
   ],
   "source": [
    "#I don't need to reload these if we run it all in the same notebook but I'm pasting the load functions here anyway\n",
    "image_train = np.load(config[\"image_feat_path\"])\n",
    "caption_train = np.load(config[\"caption_feat_path\"])\n",
    "cap2img_train = np.load(config[\"caption_to_image_path\"])\n",
    "\n",
    "image_val = np.load(config[\"val_image_feat_path\"])\n",
    "caption_val = np.load(config[\"val_caption_feat_path\"])\n",
    "cap2img_val = np.load(config[\"val_caption_to_image_path\"])\n",
    "\n",
    "print(\"Shapes (train):\", image_train.shape, caption_train.shape, cap2img_train.shape)\n",
    "print(\"Shapes (val):\", image_val.shape, caption_val.shape, cap2img_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f2533cba-11b9-4af5-96d0-dc946ff30509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Compute train-set normalization (mean/std) and apply to all splits\n",
    "# Normalize per-feature (column-wise) using training set statistics\n",
    "img_mean = image_train.mean(axis=0, keepdims=True)\n",
    "img_std = image_train.std(axis=0, keepdims=True) + 1e-6\n",
    "\n",
    "txt_mean = caption_train.mean(axis=0, keepdims=True)\n",
    "txt_std = caption_train.std(axis=0, keepdims=True) + 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "61339446-e9f4-466f-878a-76f0a0005ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_images(x):\n",
    "    return (x - img_mean) / img_std\n",
    "\n",
    "def normalize_texts(x):\n",
    "    return (x - txt_mean) / txt_std\n",
    "\n",
    "image_train = normalize_images(image_train)\n",
    "image_val   = normalize_images(image_val)\n",
    "\n",
    "caption_train = normalize_texts(caption_train)\n",
    "caption_val   = normalize_texts(caption_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "497a8934-4ca5-405d-a1a3-ff00df0c4838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Dataset that returns paired (image_feat, caption_feat) for each caption\n",
    "class CaptionImagePairedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Iterates over captions. For index i, returns:\n",
    "      caption_features[i], image_features[ caption_to_image_idx[i] ]\n",
    "    \"\"\"\n",
    "    def __init__(self, caption_feats, image_feats, caption_to_image_idx):\n",
    "        assert len(caption_feats) == len(caption_to_image_idx)\n",
    "        self.caption_feats = caption_feats.astype(np.float32)\n",
    "        self.image_feats = image_feats.astype(np.float32)\n",
    "        self.cap2img = caption_to_image_idx.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.caption_feats)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cap = self.caption_feats[idx]\n",
    "        img = self.image_feats[self.cap2img[idx]]\n",
    "        return {\"image\": torch.from_numpy(img), \"caption\": torch.from_numpy(cap)}\n",
    "\n",
    "train_dataset = CaptionImagePairedDataset(caption_train, image_train, cap2img_train)\n",
    "val_dataset   = CaptionImagePairedDataset(caption_val, image_val, cap2img_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "093368f1-d9c7-4659-87b6-04ac4a111553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Model: two autoencoders with shared latent dimension\n",
    "class ImageAE(nn.Module):\n",
    "    def __init__(self, input_dim=2048, hidden_dim=1024, latent_dim=512):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        return z, recon\n",
    "\n",
    "class TextAE(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=512, latent_dim=512):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        return z, recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "23ba263b-8371-4757-ae68-1212544e9782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate models\n",
    "img_ae = ImageAE(\n",
    "    input_dim=config[\"img_input_dim\"],\n",
    "    hidden_dim=config[\"img_hidden\"],\n",
    "    latent_dim=config[\"latent_dim\"]\n",
    ").to(device)\n",
    "\n",
    "txt_ae = TextAE(\n",
    "    input_dim=config[\"txt_input_dim\"],\n",
    "    hidden_dim=config[\"txt_hidden\"],\n",
    "    latent_dim=config[\"latent_dim\"]\n",
    ").to(device)\n",
    "\n",
    "# 6) Losses and optimizer\n",
    "recon_loss_fn = nn.MSELoss()    # reconstruction for both\n",
    "align_loss_fn = nn.MSELoss()    # align latents\n",
    "\n",
    "params = list(img_ae.parameters()) + list(txt_ae.parameters())\n",
    "optimizer = Adam(params, lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c2837db3-a3a4-4e93-b0b6-2c3025543674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Training / validation loop\n",
    "def run_epoch(loader, training=True):\n",
    "    if training:\n",
    "        img_ae.train(); txt_ae.train()\n",
    "    else:\n",
    "        img_ae.eval(); txt_ae.eval()\n",
    "\n",
    "    total_recon_img = 0.0\n",
    "    total_recon_txt = 0.0\n",
    "    total_align = 0.0\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"train\" if training else \"val\")\n",
    "    with torch.set_grad_enabled(training):\n",
    "        for batch in pbar:\n",
    "            imgs = batch[\"image\"].to(device)    # shape (B, img_dim)\n",
    "            caps = batch[\"caption\"].to(device)  # shape (B, txt_dim)\n",
    "            batch_size = imgs.shape[0]\n",
    "\n",
    "            # forward\n",
    "            z_img, img_recon = img_ae(imgs)\n",
    "            z_txt, txt_recon = txt_ae(caps)\n",
    "\n",
    "            # losses\n",
    "            L_img = recon_loss_fn(img_recon, imgs)\n",
    "            L_txt = recon_loss_fn(txt_recon, caps)\n",
    "            L_align = align_loss_fn(z_img, z_txt)\n",
    "\n",
    "            loss = L_img + L_txt + config[\"lambda_align\"] * L_align\n",
    "\n",
    "            if training:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_recon_img += L_img.item() * batch_size\n",
    "            total_recon_txt += L_txt.item() * batch_size\n",
    "            total_align += L_align.item() * batch_size\n",
    "            total_loss += loss.item() * batch_size\n",
    "            n_samples += batch_size\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{total_loss / n_samples:.4f}\",\n",
    "                \"Limg\": f\"{total_recon_img / n_samples:.4f}\",\n",
    "                \"Ltxt\": f\"{total_recon_txt / n_samples:.4f}\",\n",
    "                \"Lalign\": f\"{total_align / n_samples:.4f}\"\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / n_samples,\n",
    "        \"Limg\": total_recon_img / n_samples,\n",
    "        \"Ltxt\": total_recon_txt / n_samples,\n",
    "        \"Lalign\": total_align / n_samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dc69c05c-7a7c-49bd-b742-eccc591b52a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:05<00:00, 42.26it/s, loss=0.9268, Limg=0.4700, Ltxt=0.3571, Lalign=0.0997]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 116.35it/s, loss=0.6464, Limg=0.3226, Ltxt=0.2340, Lalign=0.0897]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.9268 | Val loss: 0.6464\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 2/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 47.15it/s, loss=0.5338, Limg=0.2490, Ltxt=0.2050, Lalign=0.0798]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 114.00it/s, loss=0.5140, Limg=0.2508, Ltxt=0.1895, Lalign=0.0738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5338 | Val loss: 0.5140\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 3/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 46.68it/s, loss=0.4416, Limg=0.2014, Ltxt=0.1732, Lalign=0.0670]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 110.97it/s, loss=0.4616, Limg=0.2276, Ltxt=0.1696, Lalign=0.0643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4416 | Val loss: 0.4616\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 4/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 47.91it/s, loss=0.3897, Limg=0.1776, Ltxt=0.1530, Lalign=0.0592]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 130.01it/s, loss=0.4230, Limg=0.2118, Ltxt=0.1534, Lalign=0.0578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3897 | Val loss: 0.4230\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 5/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 48.13it/s, loss=0.3593, Limg=0.1646, Ltxt=0.1404, Lalign=0.0543]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 109.35it/s, loss=0.3924, Limg=0.2004, Ltxt=0.1386, Lalign=0.0534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3593 | Val loss: 0.3924\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 6/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 47.70it/s, loss=0.3339, Limg=0.1543, Ltxt=0.1290, Lalign=0.0506]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 111.17it/s, loss=0.3802, Limg=0.1980, Ltxt=0.1317, Lalign=0.0504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3339 | Val loss: 0.3802\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 7/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 48.45it/s, loss=0.3163, Limg=0.1485, Ltxt=0.1197, Lalign=0.0481]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 111.88it/s, loss=0.3626, Limg=0.1934, Ltxt=0.1213, Lalign=0.0479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3163 | Val loss: 0.3626\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 8/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 47.10it/s, loss=0.3018, Limg=0.1432, Ltxt=0.1124, Lalign=0.0462]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 97.88it/s, loss=0.3533, Limg=0.1909, Ltxt=0.1161, Lalign=0.0464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3018 | Val loss: 0.3533\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 9/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 46.66it/s, loss=0.2883, Limg=0.1386, Ltxt=0.1050, Lalign=0.0447]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 110.59it/s, loss=0.3379, Limg=0.1872, Ltxt=0.1061, Lalign=0.0446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2883 | Val loss: 0.3379\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 10/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 48.29it/s, loss=0.2769, Limg=0.1345, Ltxt=0.0989, Lalign=0.0434]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 111.83it/s, loss=0.3334, Limg=0.1871, Ltxt=0.1027, Lalign=0.0435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2769 | Val loss: 0.3334\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 11/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 48.35it/s, loss=0.2675, Limg=0.1315, Ltxt=0.0935, Lalign=0.0426]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 106.79it/s, loss=0.3213, Limg=0.1851, Ltxt=0.0937, Lalign=0.0425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2675 | Val loss: 0.3213\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 12/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 48.08it/s, loss=0.2609, Limg=0.1297, Ltxt=0.0893, Lalign=0.0420]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 108.92it/s, loss=0.3142, Limg=0.1839, Ltxt=0.0879, Lalign=0.0423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2609 | Val loss: 0.3142\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 13/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 48.53it/s, loss=0.2524, Limg=0.1264, Ltxt=0.0847, Lalign=0.0413]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 114.46it/s, loss=0.3065, Limg=0.1790, Ltxt=0.0861, Lalign=0.0414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2524 | Val loss: 0.3065\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 14/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 46.92it/s, loss=0.2467, Limg=0.1249, Ltxt=0.0810, Lalign=0.0407]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 86.83it/s, loss=0.3114, Limg=0.1811, Ltxt=0.0892, Lalign=0.0411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2467 | Val loss: 0.3114\n",
      "\n",
      "=== Epoch 15/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 47.60it/s, loss=0.2414, Limg=0.1230, Ltxt=0.0780, Lalign=0.0404]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 114.80it/s, loss=0.3132, Limg=0.1773, Ltxt=0.0954, Lalign=0.0404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2414 | Val loss: 0.3132\n",
      "\n",
      "=== Epoch 16/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 48.51it/s, loss=0.2366, Limg=0.1204, Ltxt=0.0762, Lalign=0.0400]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 111.87it/s, loss=0.3002, Limg=0.1755, Ltxt=0.0844, Lalign=0.0403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2366 | Val loss: 0.3002\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 17/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 48.14it/s, loss=0.2324, Limg=0.1199, Ltxt=0.0727, Lalign=0.0399]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 106.69it/s, loss=0.2845, Limg=0.1731, Ltxt=0.0716, Lalign=0.0398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2324 | Val loss: 0.2845\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 18/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 47.54it/s, loss=0.2267, Limg=0.1163, Ltxt=0.0710, Lalign=0.0393]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 119.36it/s, loss=0.2853, Limg=0.1747, Ltxt=0.0710, Lalign=0.0396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2267 | Val loss: 0.2853\n",
      "\n",
      "=== Epoch 19/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 48.02it/s, loss=0.2242, Limg=0.1165, Ltxt=0.0685, Lalign=0.0392]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 113.21it/s, loss=0.2837, Limg=0.1734, Ltxt=0.0709, Lalign=0.0394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2242 | Val loss: 0.2837\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 20/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 46.76it/s, loss=0.2215, Limg=0.1147, Ltxt=0.0676, Lalign=0.0392]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 97.89it/s, loss=0.2884, Limg=0.1777, Ltxt=0.0712, Lalign=0.0395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2215 | Val loss: 0.2884\n",
      "\n",
      "=== Epoch 21/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:05<00:00, 44.35it/s, loss=0.2202, Limg=0.1149, Ltxt=0.0661, Lalign=0.0393]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 117.63it/s, loss=0.2816, Limg=0.1757, Ltxt=0.0666, Lalign=0.0393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2202 | Val loss: 0.2816\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 22/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 47.45it/s, loss=0.2154, Limg=0.1120, Ltxt=0.0644, Lalign=0.0389]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 113.98it/s, loss=0.2793, Limg=0.1730, Ltxt=0.0673, Lalign=0.0390]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2154 | Val loss: 0.2793\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 23/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 47.42it/s, loss=0.2136, Limg=0.1108, Ltxt=0.0641, Lalign=0.0388]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 104.69it/s, loss=0.2799, Limg=0.1739, Ltxt=0.0668, Lalign=0.0393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2136 | Val loss: 0.2799\n",
      "\n",
      "=== Epoch 24/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 46.82it/s, loss=0.2100, Limg=0.1103, Ltxt=0.0609, Lalign=0.0387]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 110.96it/s, loss=0.2750, Limg=0.1729, Ltxt=0.0633, Lalign=0.0388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2100 | Val loss: 0.2750\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 25/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 47.69it/s, loss=0.2114, Limg=0.1113, Ltxt=0.0612, Lalign=0.0389]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 102.85it/s, loss=0.2793, Limg=0.1690, Ltxt=0.0711, Lalign=0.0393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2114 | Val loss: 0.2793\n",
      "\n",
      "=== Epoch 26/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 48.80it/s, loss=0.2079, Limg=0.1082, Ltxt=0.0611, Lalign=0.0387]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 122.12it/s, loss=0.2748, Limg=0.1705, Ltxt=0.0651, Lalign=0.0392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2079 | Val loss: 0.2748\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 27/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 49.03it/s, loss=0.2057, Limg=0.1080, Ltxt=0.0591, Lalign=0.0386]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 130.02it/s, loss=0.2739, Limg=0.1708, Ltxt=0.0641, Lalign=0.0390]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2057 | Val loss: 0.2739\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 28/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 49.24it/s, loss=0.2045, Limg=0.1078, Ltxt=0.0580, Lalign=0.0387]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 138.62it/s, loss=0.2735, Limg=0.1696, Ltxt=0.0650, Lalign=0.0389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2045 | Val loss: 0.2735\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 29/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 48.66it/s, loss=0.2020, Limg=0.1054, Ltxt=0.0580, Lalign=0.0385]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 109.11it/s, loss=0.2719, Limg=0.1698, Ltxt=0.0633, Lalign=0.0389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2020 | Val loss: 0.2719\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 30/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 47.33it/s, loss=0.2016, Limg=0.1053, Ltxt=0.0579, Lalign=0.0385]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 115.56it/s, loss=0.2679, Limg=0.1685, Ltxt=0.0606, Lalign=0.0388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2016 | Val loss: 0.2679\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 31/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 47.84it/s, loss=0.2002, Limg=0.1053, Ltxt=0.0563, Lalign=0.0385]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 125.28it/s, loss=0.2698, Limg=0.1701, Ltxt=0.0609, Lalign=0.0388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2002 | Val loss: 0.2698\n",
      "\n",
      "=== Epoch 32/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 47.63it/s, loss=0.1970, Limg=0.1033, Ltxt=0.0554, Lalign=0.0384]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 114.55it/s, loss=0.2625, Limg=0.1655, Ltxt=0.0586, Lalign=0.0385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1970 | Val loss: 0.2625\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 33/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 46.00it/s, loss=0.1982, Limg=0.1043, Ltxt=0.0555, Lalign=0.0385]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 104.15it/s, loss=0.2699, Limg=0.1715, Ltxt=0.0595, Lalign=0.0389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1982 | Val loss: 0.2699\n",
      "\n",
      "=== Epoch 34/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 46.78it/s, loss=0.1973, Limg=0.1030, Ltxt=0.0558, Lalign=0.0385]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 118.48it/s, loss=0.2631, Limg=0.1671, Ltxt=0.0572, Lalign=0.0387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1973 | Val loss: 0.2631\n",
      "\n",
      "=== Epoch 35/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 47.34it/s, loss=0.1950, Limg=0.1030, Ltxt=0.0535, Lalign=0.0385]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 110.22it/s, loss=0.2648, Limg=0.1681, Ltxt=0.0580, Lalign=0.0387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1950 | Val loss: 0.2648\n",
      "\n",
      "=== Epoch 36/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 46.88it/s, loss=0.1928, Limg=0.1009, Ltxt=0.0536, Lalign=0.0382]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 119.23it/s, loss=0.2637, Limg=0.1667, Ltxt=0.0584, Lalign=0.0386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1928 | Val loss: 0.2637\n",
      "\n",
      "=== Epoch 37/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 47.52it/s, loss=0.1929, Limg=0.1010, Ltxt=0.0536, Lalign=0.0383]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 116.62it/s, loss=0.2615, Limg=0.1643, Ltxt=0.0586, Lalign=0.0385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1929 | Val loss: 0.2615\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 38/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 47.45it/s, loss=0.1920, Limg=0.1010, Ltxt=0.0527, Lalign=0.0383]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 114.42it/s, loss=0.2672, Limg=0.1679, Ltxt=0.0605, Lalign=0.0388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1920 | Val loss: 0.2672\n",
      "\n",
      "=== Epoch 39/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 48.06it/s, loss=0.1927, Limg=0.1002, Ltxt=0.0541, Lalign=0.0384]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 123.06it/s, loss=0.2606, Limg=0.1656, Ltxt=0.0562, Lalign=0.0389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1927 | Val loss: 0.2606\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 40/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 222/222 [00:04<00:00, 48.57it/s, loss=0.1897, Limg=0.0996, Ltxt=0.0518, Lalign=0.0383]\n",
      "val: 100%|██████████| 48/48 [00:00<00:00, 119.64it/s, loss=0.2623, Limg=0.1653, Ltxt=0.0581, Lalign=0.0388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1897 | Val loss: 0.2623\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, config[\"epochs\"] + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{config['epochs']} ===\")\n",
    "    train_metrics = run_epoch(train_loader, training=True)\n",
    "    val_metrics = run_epoch(val_loader, training=False)\n",
    "\n",
    "    print(f\"Train loss: {train_metrics['loss']:.4f} | Val loss: {val_metrics['loss']:.4f}\")\n",
    "\n",
    "    # Save checkpoint (every epoch)\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch,\n",
    "        \"img_state\": img_ae.state_dict(),\n",
    "        \"txt_state\": txt_ae.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"train_metrics\": train_metrics,\n",
    "        \"val_metrics\": val_metrics,\n",
    "        \"config\": config\n",
    "    }\n",
    "    ckpt_path = os.path.join(config[\"checkpoint_dir\"], f\"corr_ae_epoch{epoch}.pt\")\n",
    "    torch.save(ckpt, ckpt_path)\n",
    "\n",
    "    # Keep best\n",
    "    if val_metrics[\"loss\"] < best_val_loss:\n",
    "        best_val_loss = val_metrics[\"loss\"]\n",
    "        torch.save(ckpt, os.path.join(config[\"checkpoint_dir\"], \"corr_ae_best.pt\"))\n",
    "        print(\"Saved best checkpoint.\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d6522d1f-3127-44bc-b34f-1398e3938c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best checkpoint from ./corr_ae_checkpoints\\corr_ae_best.pt (epoch 39)\n"
     ]
    }
   ],
   "source": [
    "#Now time to evaluate on the validation set\n",
    "#Probably don't need to reload the model, but I'm going to include the code again in case we break this up into more managable files\n",
    "# --- Load best checkpoint ---\n",
    "best_ckpt_path = os.path.join(config[\"checkpoint_dir\"], \"corr_ae_best.pt\")\n",
    "ckpt = torch.load(best_ckpt_path, map_location=device)\n",
    "\n",
    "img_ae.load_state_dict(ckpt[\"img_state\"])\n",
    "txt_ae.load_state_dict(ckpt[\"txt_state\"])\n",
    "img_ae.eval(); txt_ae.eval()\n",
    "\n",
    "print(f\"Loaded best checkpoint from {best_ckpt_path} (epoch {ckpt['epoch']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6ded49cc-65d2-4e10-8403-155c03e9eb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded latent shapes: (1214, 512) (6070, 512)\n"
     ]
    }
   ],
   "source": [
    "# Encode into latent space\n",
    "with torch.no_grad():\n",
    "    # Encode images\n",
    "    Z_imgs = []\n",
    "    for i in range(0, image_val.shape[0], 256):\n",
    "        batch = torch.from_numpy(image_val[i:i+256]).float().to(device)\n",
    "        z, _ = img_ae(batch)\n",
    "        Z_imgs.append(z.cpu().numpy())\n",
    "    Z_imgs = np.concatenate(Z_imgs, axis=0)   # shape (N_images, latent_dim)\n",
    "\n",
    "    # Encode captions\n",
    "    Z_caps = []\n",
    "    for i in range(0, caption_val.shape[0], 256):\n",
    "        batch = torch.from_numpy(caption_val[i:i+256]).float().to(device)\n",
    "        z, _ = txt_ae(batch)\n",
    "        Z_caps.append(z.cpu().numpy())\n",
    "    Z_caps = np.concatenate(Z_caps, axis=0)   # shape (N_captions, latent_dim)\n",
    "\n",
    "print(\"Encoded latent shapes:\", Z_imgs.shape, Z_caps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d2564e05-cb4d-4429-be2b-871e89bc3722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1: 0.0524\n",
      "Recall@5: 0.1633\n",
      "Recall@10: 0.2540\n",
      "MedianRank: 38.0000\n"
     ]
    }
   ],
   "source": [
    "#Use Recall@1/5/10 to evaluate hyperparameter performance\n",
    "#Note that we are using cosine similarity\n",
    "#Should we consider using L2 metric instead? Does this even make sense?\n",
    "def retrieval_metrics(Z_caps, Z_imgs, caption_to_image_idx):\n",
    "    sims = cosine_similarity(Z_caps, Z_imgs)  # (num_caps, num_imgs)\n",
    "    ranks = []\n",
    "    for i, true_img_idx in enumerate(caption_to_image_idx):\n",
    "        sim_scores = sims[i]\n",
    "        sorted_indices = np.argsort(-sim_scores)  # descending\n",
    "        rank = np.where(sorted_indices == true_img_idx)[0][0] + 1\n",
    "        ranks.append(rank)\n",
    "\n",
    "    ranks = np.array(ranks)\n",
    "    recall_at_1  = np.mean(ranks <= 1)\n",
    "    recall_at_5  = np.mean(ranks <= 5)\n",
    "    recall_at_10 = np.mean(ranks <= 10)\n",
    "    med_rank = np.median(ranks)\n",
    "\n",
    "    return {\n",
    "        \"Recall@1\": recall_at_1,\n",
    "        \"Recall@5\": recall_at_5,\n",
    "        \"Recall@10\": recall_at_10,\n",
    "        \"MedianRank\": med_rank\n",
    "    }\n",
    "\n",
    "metrics_val = retrieval_metrics(Z_caps, Z_imgs, cap2img_val)\n",
    "for k, v in metrics_val.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b95e75e4-ba15-4a36-aed3-86924673b338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick visualization of what images are retrieved by what caption:\n",
    "image_dir = \"path/to/Flickr8k/Images\"  # e.g. \"/content/Flickr8k/Images\"\n",
    "\n",
    "def show_top_images_for_caption(caption_idx, top_k=5):\n",
    "    \"\"\"\n",
    "    Show top-k retrieved validation images for a given caption index.\n",
    "    \"\"\"\n",
    "    # Get the embedding for this caption\n",
    "    caption_embedding = Z_caps[caption_idx].reshape(1, -1)\n",
    "    sims = cosine_similarity(caption_embedding, Z_imgs)[0]\n",
    "    top_img_indices = np.argsort(-sims)[:top_k]\n",
    "\n",
    "    # Print caption text\n",
    "    print(f\"\\nCAPTION: {caption_val[caption_idx]}\")\n",
    "    true_img_idx = cap2img_val[caption_idx]\n",
    "    print(f\"TRUE IMAGE: {image_val[true_img_idx]} (index {true_img_idx})\")\n",
    "    print(f\"Top {top_k} retrieved images:\")\n",
    "\n",
    "    # Display images\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    for i, img_idx in enumerate(top_img_indices):\n",
    "        img_name = image_val[img_idx]  # this assumes image_val contains filenames\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not open {img_path}: {e}\")\n",
    "            continue\n",
    "        plt.subplot(1, top_k, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Rank {i+1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "29eade03-1612-4ab1-9d94-aab8ee44eecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CAPTION: [-4.40105468e-01  2.62257129e-01 -1.82232273e+00  1.71197605e+00\n",
      "  1.61076617e+00  1.87989962e+00 -1.62736058e+00  1.18145227e+00\n",
      " -2.05603933e+00  1.02855766e+00  1.01329672e+00 -7.16462910e-01\n",
      " -9.71074820e-01  8.97090316e-01 -3.40079486e-01  6.10017776e-01\n",
      " -9.27111983e-01 -1.01340437e+00 -4.90179360e-01 -2.48719901e-01\n",
      " -4.23995793e-01  1.04763293e+00 -1.71742356e+00  6.69775307e-01\n",
      " -4.73519862e-01  1.22269928e+00  6.58022404e-01 -8.49948704e-01\n",
      "  1.39369681e-01  1.06526160e+00  3.10615599e-01 -1.74752653e+00\n",
      "  7.66426086e-01  6.06213868e-01 -3.13661933e-01 -1.16403329e+00\n",
      " -1.45711675e-01  2.63895333e-01 -1.61355817e+00 -9.26913738e-01\n",
      "  1.97226977e+00  3.24242890e-01 -5.30107796e-01 -1.21021128e+00\n",
      "  1.24355778e-01 -5.41140616e-01  5.43014467e-01  9.64143932e-01\n",
      "  1.34140587e+00  8.16520154e-01 -7.91564763e-01 -5.19880056e-01\n",
      "  5.68725288e-01  3.08898352e-02  6.28872156e-01  1.61347345e-01\n",
      "  1.35990477e+00 -6.41580582e-01  2.30055881e+00  4.66104656e-01\n",
      " -3.53251338e-01 -1.06038857e+00  1.44811526e-01 -8.13110769e-01\n",
      " -1.20353186e+00 -2.63761759e-01  9.42643464e-01  8.95047724e-01\n",
      "  1.75632620e+00  7.43454814e-01  5.56372106e-01  3.42844278e-02\n",
      " -2.59994507e-01  3.66265357e-01 -8.97096336e-01 -2.19378972e+00\n",
      "  1.04678404e+00 -1.40525192e-01 -1.24035704e+00 -3.14771026e-01\n",
      "  7.97742724e-01 -1.09948874e+00  3.25165629e-01 -9.40770090e-01\n",
      " -2.48917073e-01  1.37865293e+00 -3.50159794e-01  1.51324302e-01\n",
      "  2.27893949e-01  4.06833023e-01  3.11595410e-01  5.23294568e-01\n",
      " -7.10626125e-01 -4.73897547e-01 -1.49985158e+00  1.40091383e+00\n",
      " -1.41242862e+00  1.24192297e+00 -1.38522148e+00  2.75744677e-01\n",
      " -3.03077370e-01  3.07765454e-01 -1.00266194e+00 -2.57188141e-01\n",
      " -5.19409478e-01 -5.90111434e-01  1.05737376e+00 -1.28670096e+00\n",
      "  1.16066647e+00  5.69782197e-01 -1.67165911e+00 -1.21621251e-01\n",
      " -1.47275954e-01  3.13536495e-01  6.06952548e-01  8.62276554e-01\n",
      " -2.43828988e+00 -5.06788254e-01 -5.88939667e-01  1.74490011e+00\n",
      " -2.32916325e-01 -6.41960680e-01  3.70563865e-01 -3.00058067e-01\n",
      "  1.23238826e+00 -7.24473417e-01 -1.16189599e+00 -1.08719498e-01\n",
      " -1.81091440e+00 -1.31098700e+00 -1.70385945e+00  4.57821965e-01\n",
      " -6.40430689e-01 -7.08196163e-01  1.17502189e+00 -4.58044857e-01\n",
      " -7.62684584e-01 -8.98837924e-01  9.16589200e-01 -4.97618556e-01\n",
      "  1.00838631e-01 -1.19592354e-01 -2.86402673e-01  9.18096490e-03\n",
      " -7.76248455e-01  5.05503677e-02  1.06335282e-01  2.11971807e+00\n",
      " -1.06649292e+00  2.96674699e-01  1.39951026e+00 -4.18106467e-01\n",
      " -1.14045911e-01  1.03201056e+00  6.82699859e-01  5.82214952e-01\n",
      " -9.77516055e-01 -8.95756900e-01 -6.76708937e-01 -5.85995793e-01\n",
      "  3.92950028e-01 -7.90648162e-01  1.74252138e-01 -9.94966745e-01\n",
      "  1.22894704e+00 -1.37867177e+00  3.17789197e-01 -5.53555787e-01\n",
      " -3.69502306e-01  5.69081716e-02  4.42608446e-01 -7.36482263e-01\n",
      "  1.32451490e-01 -1.16757607e+00 -5.47006488e-01 -1.15729427e+00\n",
      "  7.51926005e-01 -2.66151369e-01  7.82993063e-02 -2.29482695e-01\n",
      " -2.06157602e-02 -8.04958045e-01  1.72399640e+00 -1.78973162e+00\n",
      " -1.16032994e+00  9.03900445e-01 -5.21548569e-01  3.46275151e-01\n",
      "  6.50261641e-01  9.31462765e-01 -1.51360393e+00  1.29431963e-01\n",
      " -1.49801219e+00 -9.05158073e-02 -6.94907427e-01 -3.42851877e-01\n",
      " -6.89210713e-01 -4.57855940e-01  1.48772955e-01  7.35194087e-02\n",
      "  1.57430947e+00 -1.86858010e+00  2.25107849e-01  1.00722432e+00\n",
      "  8.79054248e-01 -5.51194191e-01 -6.68193579e-01 -1.15640450e+00\n",
      " -1.59730065e+00  1.56396985e+00  6.15963519e-01  3.16289216e-01\n",
      " -5.36846697e-01  4.04977322e-01 -1.45483088e+00  9.27084327e-01\n",
      " -4.70191836e-01 -5.69391906e-01 -1.34192133e+00 -1.48343527e+00\n",
      " -4.65602994e-01  7.62401104e-01  1.05434120e+00  1.16570628e+00\n",
      "  2.25427169e-02  4.18490916e-01  9.07964766e-01 -9.01468515e-01\n",
      " -2.25583658e-01  4.63633806e-01  2.18540907e+00  5.55458605e-01\n",
      " -1.68919432e+00 -7.37725258e-01  1.62918019e+00  1.63140106e+00\n",
      "  6.90432012e-01  1.08168483e-01  1.21397829e+00  4.79342401e-01\n",
      "  4.61605728e-01 -1.76146317e+00  3.32344055e-01 -1.49267316e-01\n",
      "  1.23889816e+00 -1.11346912e+00 -8.85295391e-01  1.07536483e+00\n",
      " -3.33400041e-01 -2.06038713e+00  6.14403822e-02 -8.76026869e-01\n",
      "  1.24514091e+00  4.24827754e-01  1.69669583e-01  6.41622722e-01\n",
      " -4.50942963e-01 -1.80330288e+00  6.28037274e-01  1.93449390e+00\n",
      " -8.40545595e-01  1.24504817e+00  4.88896847e-01 -1.99021494e+00\n",
      "  1.08575130e+00  1.40375102e+00 -7.84912765e-01 -1.72802174e+00\n",
      " -2.69672181e-02  5.67281902e-01  2.89906859e-01  1.49346805e+00\n",
      " -6.27446592e-01 -2.24115387e-01 -9.06106651e-01  1.33450508e+00\n",
      " -1.83427382e+00 -1.44770575e+00  6.25925004e-01  8.20615441e-02\n",
      " -1.61856741e-01  8.17893863e-01 -3.95308733e-01  1.25374115e+00\n",
      " -9.53299463e-01  4.47494715e-01 -8.89451146e-01 -6.77341297e-02\n",
      "  5.84442854e-01  1.48810518e+00  1.24004745e+00  1.74443269e+00\n",
      "  4.54495072e-01 -9.54226255e-01  8.81199121e-01  1.86351848e+00\n",
      " -1.10983241e+00 -9.08106193e-02 -4.25014824e-01 -1.89155829e+00\n",
      " -1.07021511e+00 -3.78564835e-01  1.58352518e+00  6.85489833e-01\n",
      " -6.95380792e-02  1.55064571e+00  1.02366340e+00 -7.08514750e-01\n",
      " -8.99548173e-01  1.48608482e+00 -7.17062414e-01 -6.97335482e-01\n",
      " -1.55997053e-01 -1.25316493e-02 -1.09442770e+00  8.60587060e-01\n",
      "  2.64221478e+00 -2.37933040e+00  7.18563497e-01  6.94591880e-01\n",
      " -8.04858029e-01 -9.02381837e-01  9.53761101e-01 -7.04851523e-02\n",
      " -2.50099599e-02  6.58784807e-01 -6.33433223e-01  1.92712203e-01\n",
      " -5.14530778e-01 -2.14282677e-01  3.19924176e-01  5.05476713e-01\n",
      " -1.04705954e+00  5.27813613e-01 -4.51637566e-01  4.69633609e-01\n",
      " -1.51175737e+00 -8.72759819e-01 -2.18398690e+00 -3.00060481e-01\n",
      "  3.43637764e-01  1.37857959e-01  1.71931863e+00  1.39628637e+00\n",
      " -1.17950952e+00  2.30690464e-01 -7.22274244e-01 -1.24452794e+00\n",
      "  8.42181265e-01  1.75357691e-03 -1.39066148e+00 -1.17422330e+00\n",
      " -4.44684207e-01  4.37025011e-01  6.18523538e-01 -1.53818130e+00\n",
      " -1.63157701e-01  1.65879965e+00 -1.47616422e+00  1.42470264e+00\n",
      " -2.56297022e-01  1.19525000e-01 -8.27743232e-01 -8.89400542e-01\n",
      " -1.19981396e+00 -9.98251379e-01 -7.32706964e-01 -5.83330214e-01\n",
      "  8.07750344e-01 -5.93199804e-02 -1.67847133e+00  6.23096526e-01\n",
      "  9.53857228e-02  6.81865513e-01  6.56894922e-01  1.45521313e-01\n",
      " -2.52596378e-01  7.27607727e-01  8.65915179e-01 -7.73631275e-01\n",
      " -2.79961765e-01  1.85242283e+00  1.50627804e+00  9.42980647e-01\n",
      "  1.80302155e+00 -1.10474944e+00  5.83567202e-01 -2.52924180e+00\n",
      " -4.35793489e-01  2.26535529e-01  5.34317076e-01  1.10492754e+00\n",
      " -5.93376398e-01  1.69406343e+00 -2.13750839e+00 -1.05710113e+00\n",
      " -4.49074835e-01 -7.80206144e-01 -2.74377525e-01 -9.30759162e-02\n",
      "  1.14021383e-01  7.32915342e-01  8.82475257e-01 -4.75703031e-01\n",
      "  1.52453089e+00 -2.76596040e-01 -1.65561581e+00  4.28159118e-01\n",
      "  5.28905332e-01  7.52493218e-02  1.23033488e+00  3.13308299e-01\n",
      "  2.86908507e-01  2.55386019e+00  9.34615970e-01 -2.70449936e-01\n",
      " -5.58148265e-01  1.72762072e+00 -2.47887224e-01  6.16717100e-01\n",
      "  1.54659402e+00 -7.80167401e-01 -7.69690573e-01 -3.77578646e-01\n",
      " -5.89415729e-01  1.35415921e-03  6.09124303e-01 -2.62102336e-01\n",
      " -3.86887014e-01  3.84366840e-01 -5.36455452e-01  8.77530098e-01\n",
      " -5.06978810e-01 -1.42068005e+00 -5.15402257e-01 -1.10052474e-01\n",
      "  1.54438671e-02 -2.71238565e-01  5.42567611e-01  2.00638938e+00\n",
      " -5.69214523e-01  6.94065869e-01 -8.46766829e-01  7.56741464e-01\n",
      " -7.44386792e-01 -1.98292422e+00  7.60350302e-02  7.35206902e-01\n",
      " -1.06537974e+00  9.21976328e-01  2.65227467e-01  5.12110531e-01\n",
      "  7.75954723e-01 -7.73210049e-01  1.21030676e+00  1.76763430e-01\n",
      "  1.24487376e+00 -9.37829494e-01  7.19772339e-01  8.72755468e-01\n",
      " -9.23342854e-02  3.37737232e-01 -1.63258672e+00 -1.77149639e-01\n",
      " -3.54874879e-02  1.10145211e+00  1.92884326e+00 -7.85410881e-01\n",
      "  2.27048063e+00  4.83407587e-01  9.83829498e-01  2.23644897e-01\n",
      " -9.65174675e-01  1.10295630e+00  9.04264092e-01 -7.28987038e-01\n",
      "  9.42520976e-01 -6.84102178e-01 -2.38167956e-01  5.52315861e-02\n",
      "  1.84116328e+00  1.52910337e-01 -1.43045282e+00  1.68119359e+00\n",
      "  8.08990121e-01  1.04176784e+00 -3.45390111e-01  1.79149759e+00\n",
      "  1.52393377e+00  1.87773898e-01 -2.06497774e-01  1.05412316e+00\n",
      "  2.98161626e-01  1.12612176e+00 -1.23279011e+00  1.46586788e+00\n",
      "  1.53595161e+00  1.27833843e+00  2.37803388e+00 -3.73186201e-01\n",
      " -7.39110336e-02  9.43414450e-01 -1.86082470e+00  1.11420369e+00\n",
      " -1.23277962e+00 -1.93367493e+00  1.64296353e+00 -8.58401597e-01\n",
      " -1.69992924e+00 -4.55802798e-01  1.08875178e-01 -1.94896743e-01\n",
      " -2.03018379e+00  1.86800584e-01  7.30402172e-01 -4.59260970e-01\n",
      "  3.37780088e-01 -3.42681170e-01 -9.68107283e-01  5.84402323e-01\n",
      " -1.04348397e+00 -1.36722028e-01  1.22609842e+00  1.47224510e+00\n",
      "  5.87766886e-01  1.86172992e-01 -2.82632828e-01 -8.58941615e-01\n",
      "  9.76465464e-01 -2.57275254e-02 -5.16648650e-01 -9.94041979e-01\n",
      " -1.87280267e-01  1.09403753e+00  3.20066899e-01  1.21488772e-01\n",
      " -2.94659555e-01  1.00586641e+00 -2.25027919e-01  2.22779825e-01\n",
      " -6.86902225e-01 -1.93127513e-01  1.27852190e+00 -6.14427507e-01\n",
      "  3.14210743e-01  4.30988908e-01  1.09416497e+00  1.86822820e+00\n",
      "  1.61501372e+00 -5.20404935e-01  1.41360688e+00  4.67033833e-02\n",
      "  6.73933744e-01  8.14239979e-01 -4.86697108e-01  7.65567183e-01\n",
      "  1.36222553e+00  1.15447450e+00 -3.88733208e-01 -1.58380187e+00\n",
      "  2.70571291e-01  7.07344189e-02  7.41478682e-01 -4.14286144e-02\n",
      " -1.05246174e+00  3.53726596e-01 -8.88733327e-01 -3.16267222e-01\n",
      " -7.99724996e-01  1.50797391e+00 -1.98301375e+00  5.85993588e-01\n",
      " -4.05172676e-01 -4.37568069e-01  1.08977437e+00  3.86254251e-01\n",
      "  1.46980536e+00 -7.12801278e-01  2.33824730e+00  8.46677303e-01\n",
      "  1.52024508e+00  1.85307360e+00 -9.94625866e-01 -2.46424824e-01\n",
      "  1.81908000e+00  6.63963437e-01  4.85879660e-01 -3.73425722e-01\n",
      " -1.21853125e+00 -2.93718547e-01  1.58441067e+00  2.35082358e-01\n",
      " -1.34997606e+00 -5.61141193e-01 -4.10474002e-01 -1.07766128e+00\n",
      "  1.16303527e+00 -8.75799119e-01  5.75253904e-01 -7.83663318e-02\n",
      " -1.68517418e-02 -1.19093823e+00 -1.67188144e+00 -3.02393198e-01\n",
      " -1.46913397e+00  1.19635664e-01  8.51606965e-01 -8.50359797e-01\n",
      " -2.58483421e-02  4.13999647e-01  1.61328483e+00 -5.73450983e-01\n",
      "  4.61783186e-02 -2.38622457e-01 -5.76809868e-02 -1.03519213e+00\n",
      " -4.93555158e-01 -1.62391543e+00  1.09420812e+00 -2.49408230e-01\n",
      "  5.31753302e-01  3.32813635e-02  5.10955989e-01  8.82743061e-01\n",
      "  4.95729268e-01 -3.19692463e-01 -1.73135042e-01  3.64729017e-01\n",
      "  6.90889835e-01  6.79942846e-01 -3.21919739e-01 -7.10435271e-01\n",
      " -5.47328472e-01  2.60909885e-01 -5.60448706e-01 -4.73131895e-01\n",
      "  7.47562766e-01  8.75013113e-01 -1.26233387e+00  8.61553848e-01\n",
      " -1.31055221e-01 -2.53220677e-01 -1.79969621e+00  7.67150998e-01\n",
      " -1.15812957e+00 -8.48348498e-01 -6.87268302e-02  3.85300033e-02\n",
      "  5.29633239e-02  7.63327718e-01 -5.14717519e-01  5.50649166e-01\n",
      "  1.05882561e+00  8.26799631e-01  5.51937103e-01 -8.22882116e-01\n",
      " -5.19596636e-01  1.86315417e-01  5.26300192e-01  4.91208613e-01\n",
      "  7.89817512e-01 -1.85820782e+00 -5.50440669e-01 -4.34840828e-01\n",
      "  9.54882860e-01 -1.25822151e+00  4.20853704e-01  2.35061437e-01\n",
      " -1.40514421e+00 -4.39639777e-01 -6.97094560e-01  3.12654823e-01\n",
      " -8.59930098e-01  1.26815176e+00 -1.31857884e+00 -1.55871940e+00\n",
      " -3.24439675e-01  1.83369756e-01  7.64867246e-01 -1.42900646e+00\n",
      " -4.59039845e-02 -1.03777969e+00  4.45320129e-01 -1.65023017e+00\n",
      "  1.75485146e+00 -8.32171440e-01  1.70672166e+00  5.46382785e-01\n",
      " -1.44085577e-02 -1.69212759e+00 -1.69221282e-01  4.39248741e-01\n",
      "  4.70683783e-01  2.52262235e-01  1.23864460e+00 -3.15241307e-01\n",
      " -5.71618080e-01 -9.53797162e-01 -1.30574191e+00 -2.72901297e+00\n",
      "  3.00927788e-01  6.67009592e-01 -4.58297461e-01  1.53502619e+00\n",
      " -2.02324581e+00 -7.45763779e-01 -7.36710668e-01  7.04641640e-01\n",
      "  5.26136577e-01  1.19523358e+00 -1.21223795e+00  1.72306681e+00\n",
      " -5.63616194e-02  8.70237052e-01  1.97113141e-01  3.48489314e-01\n",
      " -8.67013752e-01 -4.13648814e-01 -4.23452109e-01  2.84774542e-01\n",
      " -4.41399455e-01 -1.94969261e+00 -1.50182366e-01 -1.39367843e+00\n",
      " -1.55755126e+00  6.75323486e-01 -1.82683051e+00  3.01902205e-01\n",
      "  1.49905086e+00  9.57888722e-01  1.50191760e+00  6.58223093e-01\n",
      " -6.50965124e-02 -7.19153881e-01  7.91569471e-01  6.86691821e-01\n",
      "  7.61268318e-01  1.61064863e+00  1.13089597e+00  7.42403746e-01\n",
      "  1.16418369e-01 -7.80102491e-01 -1.80174148e+00 -1.42352057e+00\n",
      "  6.56962812e-01  1.31599379e+00  2.39994675e-01  2.51348948e+00\n",
      " -1.62222421e+00  5.31979978e-01 -1.38569862e-01 -2.12901253e-02\n",
      "  1.02568716e-01 -7.32613385e-01  6.95559204e-01 -2.01358989e-01\n",
      " -2.70112008e-01 -1.08118272e+00  1.16440487e+00 -6.85677409e-01\n",
      " -9.28649962e-01  1.30372679e+00  2.28306174e+00 -4.54872251e-01\n",
      " -1.15700793e+00  2.47414559e-02  9.88966763e-01 -1.56322205e+00]\n",
      "TRUE IMAGE: [ 0.9115711  -0.06154323  1.0609821  ... -1.069237   -0.98828775\n",
      " -0.48207498] (index 532)\n",
      "Top 5 retrieved images:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "join() argument must be str, bytes, or os.PathLike object, not 'ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(caption_val)), \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     show_top_images_for_caption(i, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[93], line 23\u001b[0m, in \u001b[0;36mshow_top_images_for_caption\u001b[1;34m(caption_idx, top_k)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, img_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(top_img_indices):\n\u001b[0;32m     22\u001b[0m     img_name \u001b[38;5;241m=\u001b[39m image_val[img_idx]  \u001b[38;5;66;03m# this assumes image_val contains filenames\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, img_name)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m         img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\n",
      "File \u001b[1;32m<frozen ntpath>:147\u001b[0m, in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n",
      "File \u001b[1;32m<frozen genericpath>:152\u001b[0m, in \u001b[0;36m_check_arg_types\u001b[1;34m(funcname, *args)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: join() argument must be str, bytes, or os.PathLike object, not 'ndarray'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in random.sample(range(len(caption_val)), 3):\n",
    "    show_top_images_for_caption(i, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa06d10-0491-4943-bc96-fb94c2575ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
