{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Reconstruction Multi-Modal Autoencoder\n\n",
    "This notebook implements the original architecture where:\n",
    "- Image features are reconstructed by the image autoencoder\n",
    "- Text features are reconstructed by the text autoencoder  \n",
    "- Latent spaces are aligned using **either MSE or contrastive loss**\n\n",
    "This forces the shared latent space to capture aligned representations.\n\n",
    "**Prerequisites:** Run `Feature_Extraction_Batch.ipynb` first to generate the required .npy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Pre-extracted Features\n\n",
    "We'll use the features already extracted from the Feature_Extraction notebook:\n",
    "- Image features: ResNet50 (2048-dim)\n",
    "- Caption features: BERT (768-dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-split data\n",
    "image_train = np.load(\"train_image_features.npy\")\n",
    "caption_train = np.load(\"train_caption_features.npy\")\n",
    "cap2img_train = np.load(\"train_caption_to_image.npy\")\n\n",
    "image_val = np.load(\"val_image_features.npy\")\n",
    "caption_val = np.load(\"val_caption_features.npy\")\n",
    "cap2img_val = np.load(\"val_caption_to_image.npy\")\n\n",
    "image_test = np.load(\"test_image_features.npy\")\n",
    "caption_test = np.load(\"test_caption_features.npy\")\n",
    "cap2img_test = np.load(\"test_caption_to_image.npy\")\n\n",
    "print(\"Train shapes:\", image_train.shape, caption_train.shape, cap2img_train.shape)\n",
    "print(\"Val shapes:\", image_val.shape, caption_val.shape, cap2img_val.shape)\n",
    "print(\"Test shapes:\", image_test.shape, caption_test.shape, cap2img_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normalize Features\n\n",
    "Normalize using training set statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute normalization parameters from training set\n",
    "img_mean = image_train.mean(axis=0, keepdims=True)\n",
    "img_std = image_train.std(axis=0, keepdims=True) + 1e-6\n\n",
    "txt_mean = caption_train.mean(axis=0, keepdims=True)\n",
    "txt_std = caption_train.std(axis=0, keepdims=True) + 1e-6\n\n",
    "# Apply normalization\n",
    "def normalize_images(x):\n",
    "    return (x - img_mean) / img_std\n\n",
    "def normalize_texts(x):\n",
    "    return (x - txt_mean) / txt_std\n\n",
    "image_train = normalize_images(image_train)\n",
    "image_val = normalize_images(image_val)\n",
    "image_test = normalize_images(image_test)\n\n",
    "caption_train = normalize_texts(caption_train)\n",
    "caption_val = normalize_texts(caption_val)\n",
    "caption_test = normalize_texts(caption_test)\n\n",
    "print(\"Normalization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset and DataLoader\n\n",
    "Pairs captions with their corresponding images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionImagePairedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Iterates over captions. For index i, returns:\n",
    "      caption_features[i], image_features[caption_to_image_idx[i]]\n",
    "    \"\"\"\n",
    "    def __init__(self, caption_feats, image_feats, caption_to_image_idx):\n",
    "        assert len(caption_feats) == len(caption_to_image_idx)\n",
    "        self.caption_feats = caption_feats.astype(np.float32)\n",
    "        self.image_feats = image_feats.astype(np.float32)\n",
    "        self.cap2img = caption_to_image_idx.astype(np.int64)\n\n",
    "    def __len__(self):\n",
    "        return len(self.caption_feats)\n\n",
    "    def __getitem__(self, idx):\n",
    "        cap = self.caption_feats[idx]\n",
    "        img = self.image_feats[self.cap2img[idx]]\n",
    "        return {\"image\": torch.from_numpy(img), \"caption\": torch.from_numpy(cap)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration\n\n",
    "**Choose your loss function here:** Set `LOSS_TYPE` to either `\"mse\"` or `\"contrastive\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CHOOSE YOUR LOSS FUNCTION\n",
    "# ============================================================\n",
    "LOSS_TYPE = \"mse\"  # Change to \"contrastive\" to use contrastive loss\n\n",
    "# Set checkpoint directory based on loss type\n",
    "if LOSS_TYPE == \"mse\":\n",
    "    checkpoint_dir = \"./corr_ae_checkpoints_mse\"\n",
    "elif LOSS_TYPE == \"contrastive\":\n",
    "    checkpoint_dir = \"./corr_ae_checkpoints_contrastive\"\n",
    "else:\n",
    "    raise ValueError(\"LOSS_TYPE must be 'mse' or 'contrastive'\")\n\n",
    "print(f\"Using {LOSS_TYPE.upper()} loss\")\n",
    "print(f\"Checkpoints: {checkpoint_dir}\")\n\n",
    "# ============================================================\n",
    "# Model Configuration\n",
    "# ============================================================\n",
    "config = {\n",
    "    \"latent_dim\": 512,\n",
    "    \"img_input_dim\": 2048,\n",
    "    \"txt_input_dim\": 768,\n",
    "    \"img_hidden\": 1024,\n",
    "    \"txt_hidden\": 512,\n",
    "    \"batch_size\": 128,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"epochs\": 40,\n",
    "    \"lambda_align\": 1.0,\n",
    "    \"temperature\": 0.07,  # Only used for contrastive loss\n",
    "    \"checkpoint_dir\": checkpoint_dir,\n",
    "    \"loss_type\": LOSS_TYPE,\n",
    "    \"seed\": 42,\n",
    "}\n\n",
    "# Set seed\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n\n",
    "# Create checkpoint directory\n",
    "os.makedirs(config[\"checkpoint_dir\"], exist_ok=True)\n\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Self-Reconstruction Autoencoder Architecture\n\n",
    "Key idea:\n",
    "- Image encoder \u2192 latent \u2192 Image decoder (outputs image features)\n",
    "- Text encoder \u2192 latent \u2192 Text decoder (outputs text features)\n\n",
    "Both autoencoders output the same latent dimension, with alignment enforced by loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAE(nn.Module):\n",
    "    \"\"\"Image autoencoder: encodes and decodes image features\"\"\"\n",
    "    def __init__(self, input_dim=2048, hidden_dim=1024, latent_dim=512):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        return z, recon\n\n\n",
    "class TextAE(nn.Module):\n",
    "    \"\"\"Text autoencoder: encodes and decodes text features\"\"\"\n",
    "    def __init__(self, input_dim=768, hidden_dim=512, latent_dim=512):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        return z, recon\n\n\n",
    "print(\"Model architecture defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(z_img, z_txt, temperature=0.07):\n",
    "    \"\"\"\n",
    "    Contrastive loss to align image and text embeddings.\n",
    "    Encourages matching pairs to be close and non-matching pairs to be far.\n",
    "    \"\"\"\n",
    "    # Normalize embeddings\n",
    "    z_img_norm = F.normalize(z_img, dim=1)\n",
    "    z_txt_norm = F.normalize(z_txt, dim=1)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    logits = torch.matmul(z_img_norm, z_txt_norm.T) / temperature\n",
    "    \n",
    "    # Labels: diagonal elements are positive pairs\n",
    "    labels = torch.arange(z_img.size(0)).to(z_img.device)\n",
    "    \n",
    "    # Cross-entropy in both directions\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "    \n",
    "    return (loss_i2t + loss_t2i) / 2\n\n\n",
    "# Reconstruction loss\n",
    "recon_loss_fn = nn.MSELoss()\n\n",
    "# Alignment loss - chosen based on LOSS_TYPE\n",
    "if config[\"loss_type\"] == \"mse\":\n",
    "    align_loss_fn = nn.MSELoss()\n",
    "    print(\"Using MSE for alignment loss\")\n",
    "else:\n",
    "    align_loss_fn = None  # Will use contrastive_loss function\n",
    "    print(\"Using contrastive loss for alignment\")\n\n",
    "print(\"Loss functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Models and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate models\n",
    "img_ae = ImageAE(\n",
    "    input_dim=config[\"img_input_dim\"],\n",
    "    hidden_dim=config[\"img_hidden\"],\n",
    "    latent_dim=config[\"latent_dim\"]\n",
    ").to(device)\n\n",
    "txt_ae = TextAE(\n",
    "    input_dim=config[\"txt_input_dim\"],\n",
    "    hidden_dim=config[\"txt_hidden\"],\n",
    "    latent_dim=config[\"latent_dim\"]\n",
    ").to(device)\n\n",
    "# Optimizer for all parameters\n",
    "params = list(img_ae.parameters()) + list(txt_ae.parameters())\n",
    "optimizer = Adam(params, lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n\n",
    "print(f\"Models initialized with {sum(p.numel() for p in params):,} total parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CaptionImagePairedDataset(caption_train, image_train, cap2img_train)\n",
    "val_dataset = CaptionImagePairedDataset(caption_val, image_val, cap2img_val)\n\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config[\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config[\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop\n\n",
    "Self-reconstruction:\n",
    "- Image \u2192 img_ae \u2192 z_img, img_recon (reconstructs itself)\n",
    "- Text \u2192 txt_ae \u2192 z_txt, txt_recon (reconstructs itself)\n\n",
    "Plus alignment loss (MSE or contrastive) to align z_img and z_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader, training=True):\n",
    "    if training:\n",
    "        img_ae.train()\n",
    "        txt_ae.train()\n",
    "    else:\n",
    "        img_ae.eval()\n",
    "        txt_ae.eval()\n\n",
    "    total_recon_img = 0.0\n",
    "    total_recon_txt = 0.0\n",
    "    total_align = 0.0\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n\n",
    "    pbar = tqdm(loader, desc=\"train\" if training else \"val\")\n",
    "    with torch.set_grad_enabled(training):\n",
    "        for batch in pbar:\n",
    "            imgs = batch[\"image\"].to(device)\n",
    "            caps = batch[\"caption\"].to(device)\n",
    "            batch_size = imgs.shape[0]\n\n",
    "            # Forward pass: self-reconstruction\n",
    "            z_img, img_recon = img_ae(imgs)\n",
    "            z_txt, txt_recon = txt_ae(caps)\n\n",
    "            # Reconstruction losses\n",
    "            L_img = recon_loss_fn(img_recon, imgs)\n",
    "            L_txt = recon_loss_fn(txt_recon, caps)\n",
    "            \n",
    "            # Alignment loss\n",
    "            if config[\"loss_type\"] == \"mse\":\n",
    "                L_align = align_loss_fn(z_img, z_txt)\n",
    "            else:  # contrastive\n",
    "                L_align = contrastive_loss(z_img, z_txt, temperature=config[\"temperature\"])\n\n",
    "            # Total loss\n",
    "            loss = L_img + L_txt + config[\"lambda_align\"] * L_align\n\n",
    "            if training:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n\n",
    "            # Accumulate metrics\n",
    "            total_recon_img += L_img.item() * batch_size\n",
    "            total_recon_txt += L_txt.item() * batch_size\n",
    "            total_align += L_align.item() * batch_size\n",
    "            total_loss += loss.item() * batch_size\n",
    "            n_samples += batch_size\n\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{total_loss / n_samples:.4f}\",\n",
    "                \"Limg\": f\"{total_recon_img / n_samples:.4f}\",\n",
    "                \"Ltxt\": f\"{total_recon_txt / n_samples:.4f}\",\n",
    "                \"Lalign\": f\"{total_align / n_samples:.4f}\"\n",
    "            })\n\n",
    "    return {\n",
    "        \"loss\": total_loss / n_samples,\n",
    "        \"Limg\": total_recon_img / n_samples,\n",
    "        \"Ltxt\": total_recon_txt / n_samples,\n",
    "        \"Lalign\": total_align / n_samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n\n",
    "for epoch in range(1, config[\"epochs\"] + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{config['epochs']} ===\")\n",
    "    train_metrics = run_epoch(train_loader, training=True)\n",
    "    val_metrics = run_epoch(val_loader, training=False)\n\n",
    "    print(f\"Train loss: {train_metrics['loss']:.4f} | Val loss: {val_metrics['loss']:.4f}\")\n\n",
    "    # Save checkpoint every epoch\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch,\n",
    "        \"img_state\": img_ae.state_dict(),\n",
    "        \"txt_state\": txt_ae.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"train_metrics\": train_metrics,\n",
    "        \"val_metrics\": val_metrics,\n",
    "        \"config\": config\n",
    "    }\n",
    "    ckpt_path = os.path.join(config[\"checkpoint_dir\"], f\"corr_ae_epoch{epoch}.pt\")\n",
    "    torch.save(ckpt, ckpt_path)\n\n",
    "    # Save best checkpoint\n",
    "    if val_metrics[\"loss\"] < best_val_loss:\n",
    "        best_val_loss = val_metrics[\"loss\"]\n",
    "        torch.save(ckpt, os.path.join(config[\"checkpoint_dir\"], \"corr_ae_best.pt\"))\n",
    "        print(\"Saved best checkpoint.\")\n\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluation: Load Best Model and Compute Retrieval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "best_ckpt_path = os.path.join(config[\"checkpoint_dir\"], \"corr_ae_best.pt\")\n",
    "ckpt = torch.load(best_ckpt_path, map_location=device)\n\n",
    "img_ae.load_state_dict(ckpt[\"img_state\"])\n",
    "txt_ae.load_state_dict(ckpt[\"txt_state\"])\n",
    "img_ae.eval()\n",
    "txt_ae.eval()\n\n",
    "print(f\"Loaded best checkpoint from epoch {ckpt['epoch']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(image_feats, caption_feats):\n",
    "    \"\"\"\n",
    "    Encode images and captions into latent space.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Encode images\n",
    "        Z_imgs = []\n",
    "        for i in range(0, image_feats.shape[0], 256):\n",
    "            batch = torch.from_numpy(image_feats[i:i+256]).float().to(device)\n",
    "            z, _ = img_ae(batch)\n",
    "            Z_imgs.append(z.cpu().numpy())\n",
    "        Z_imgs = np.concatenate(Z_imgs, axis=0)\n\n",
    "        # Encode captions\n",
    "        Z_caps = []\n",
    "        for i in range(0, caption_feats.shape[0], 256):\n",
    "            batch = torch.from_numpy(caption_feats[i:i+256]).float().to(device)\n",
    "            z, _ = txt_ae(batch)\n",
    "            Z_caps.append(z.cpu().numpy())\n",
    "        Z_caps = np.concatenate(Z_caps, axis=0)\n\n",
    "    return Z_imgs, Z_caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_metrics(Z_caps, Z_imgs, caption_to_image_idx):\n",
    "    \"\"\"\n",
    "    Compute Recall@K metrics for image retrieval given captions.\n",
    "    \"\"\"\n",
    "    sims = cosine_similarity(Z_caps, Z_imgs)\n",
    "    ranks = []\n",
    "    for i, true_img_idx in enumerate(caption_to_image_idx):\n",
    "        sim_scores = sims[i]\n",
    "        sorted_indices = np.argsort(-sim_scores)\n",
    "        rank = np.where(sorted_indices == true_img_idx)[0][0] + 1\n",
    "        ranks.append(rank)\n\n",
    "    ranks = np.array(ranks)\n",
    "    return {\n",
    "        \"Recall@1\": np.mean(ranks <= 1),\n",
    "        \"Recall@5\": np.mean(ranks <= 5),\n",
    "        \"Recall@10\": np.mean(ranks <= 10),\n",
    "        \"MedianRank\": np.median(ranks)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encoding validation set...\")\n",
    "Z_imgs_val, Z_caps_val = encode_features(image_val, caption_val)\n",
    "print(f\"Encoded shapes: {Z_imgs_val.shape}, {Z_caps_val.shape}\")\n\n",
    "metrics_val = retrieval_metrics(Z_caps_val, Z_imgs_val, cap2img_val)\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "for k, v in metrics_val.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encoding test set...\")\n",
    "Z_imgs_test, Z_caps_test = encode_features(image_test, caption_test)\n",
    "print(f\"Encoded shapes: {Z_imgs_test.shape}, {Z_caps_test.shape}\")\n\n",
    "metrics_test = retrieval_metrics(Z_caps_test, Z_imgs_test, cap2img_test)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for k, v in metrics_test.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualization (Optional)\n\n",
    "Visualize some retrieval results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load additional data for visualization\n",
    "df = pd.read_csv('./flickr8k_data/captions.txt')\n",
    "image_names = np.load('flickr8k_image_names.npy')\n\n",
    "# Recreate splits to get validation masks\n",
    "n_images = len(image_names)\n",
    "indices = np.arange(n_images)\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.30, random_state=42, shuffle=True)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42, shuffle=True)\n\n",
    "# Create masks\n",
    "caption_to_image_idx = df[\"image\"].map({name: i for i, name in enumerate(image_names)}).values.astype(int)\n",
    "val_mask = np.isin(caption_to_image_idx, val_idx)\n\n",
    "# Get validation caption texts and image names\n",
    "val_caption_texts = df[\"caption\"].values[val_mask]\n",
    "val_image_names = np.array([image_names[i] for i in val_idx])\n\n",
    "print(f\"Loaded {len(val_caption_texts)} validation captions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"./flickr8k_data/Images\"\n\n",
    "def show_top_images_for_caption(caption_idx, top_k=5):\n",
    "    \"\"\"\n",
    "    Show top-k retrieved validation images for a given caption index.\n",
    "    \"\"\"\n",
    "    caption_embedding = Z_caps_val[caption_idx].reshape(1, -1)\n",
    "    sims = cosine_similarity(caption_embedding, Z_imgs_val)[0]\n",
    "    top_img_indices = np.argsort(-sims)[:top_k]\n\n",
    "    print(f\"\\nCAPTION: {val_caption_texts[caption_idx]}\")\n",
    "    true_img_idx = cap2img_val[caption_idx]\n",
    "    print(f\"TRUE IMAGE: {val_image_names[true_img_idx]} (index {true_img_idx})\")\n",
    "    \n",
    "    plt.figure(figsize=(18, 4))\n",
    "    \n",
    "    # Show true image\n",
    "    true_img_name = val_image_names[true_img_idx]\n",
    "    true_img_path = os.path.join(image_dir, true_img_name)\n",
    "    try:\n",
    "        true_img = Image.open(true_img_path)\n",
    "        plt.subplot(1, top_k + 1, 1)\n",
    "        plt.imshow(true_img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"TRUE IMAGE\", fontweight='bold', color='green')\n",
    "    except Exception as e:\n",
    "        print(f\"Could not open true image: {e}\")\n",
    "    \n",
    "    # Show retrieved images\n",
    "    for i, img_idx in enumerate(top_img_indices):\n",
    "        img_name = val_image_names[img_idx]\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not open {img_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        plt.subplot(1, top_k + 1, i + 2)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if img_idx == true_img_idx:\n",
    "            plt.title(f\"Rank {i+1} \u2713\", fontweight='bold', color='green')\n",
    "        else:\n",
    "            plt.title(f\"Rank {i+1}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n\n",
    "# Show some random examples\n",
    "print(\"\\n=== Retrieval Examples ===\")\n",
    "for i in random.sample(range(len(Z_caps_val)), 3):\n",
    "    show_top_images_for_caption(i, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}