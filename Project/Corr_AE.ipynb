{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture 1: Self-Reconstruction Multi-Modal Autoencoder\n\n",
    "This notebook implements the original architecture where:\n",
    "- Image features are reconstructed by the image autoencoder\n",
    "- Text features are reconstructed by the text autoencoder\n",
    "- Latent spaces are aligned using contrastive loss\n\n",
    "**Prerequisites:** Run `Feature_Extraction_Batch.ipynb` first to generate the required .npy files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a56431",
   "metadata": {},
   "source": [
    "5. Multi-Modal Autoencoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a46b34b-ec97-4a65-9cb5-fb0b9ab46bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now time to set up and train the actual multi-modal autoencoder\n",
    "config = {\n",
    "    \"image_feat_path\": \"train_image_features.npy\",   # we'll load proper files below\n",
    "    \"caption_feat_path\": \"train_caption_features.npy\",\n",
    "    \"caption_to_image_path\": \"train_caption_to_image.npy\",\n",
    "    \"val_image_feat_path\": \"val_image_features.npy\",\n",
    "    \"val_caption_feat_path\": \"val_caption_features.npy\",\n",
    "    \"val_caption_to_image_path\": \"val_caption_to_image.npy\",\n",
    "    \"latent_dim\": 512,\n",
    "    \"img_input_dim\": 2048,\n",
    "    \"txt_input_dim\": 768,\n",
    "    \"img_hidden\": 1024,\n",
    "    \"txt_hidden\": 512,\n",
    "    \"batch_size\": 128,    # try 128; lower if memory limited (e.g., 64)\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"epochs\": 40,\n",
    "    \"lambda_align\": 1.0,  # weight for latent alignment loss; tuneable\n",
    "    \"checkpoint_dir\": \"./corr_ae_checkpoints_contrastive\",\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ae49527-690e-440f-babb-1fa1fe3a18ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "\n",
    "os.makedirs(config[\"checkpoint_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95c77066-c042-42ad-ba98-966847dd5981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes (train): (5663, 2048) (28315, 768) (28315,)\n",
      "Shapes (val): (1214, 2048) (6070, 768) (6070,)\n"
     ]
    }
   ],
   "source": [
    "#I don't need to reload these if we run it all in the same notebook but I'm pasting the load functions here anyway\n",
    "image_train = np.load(config[\"image_feat_path\"])\n",
    "caption_train = np.load(config[\"caption_feat_path\"])\n",
    "cap2img_train = np.load(config[\"caption_to_image_path\"])\n",
    "\n",
    "image_val = np.load(config[\"val_image_feat_path\"])\n",
    "caption_val = np.load(config[\"val_caption_feat_path\"])\n",
    "cap2img_val = np.load(config[\"val_caption_to_image_path\"])\n",
    "\n",
    "print(\"Shapes (train):\", image_train.shape, caption_train.shape, cap2img_train.shape)\n",
    "print(\"Shapes (val):\", image_val.shape, caption_val.shape, cap2img_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2533cba-11b9-4af5-96d0-dc946ff30509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Compute train-set normalization (mean/std) and apply to all splits\n",
    "# Normalize per-feature (column-wise) using training set statistics\n",
    "img_mean = image_train.mean(axis=0, keepdims=True)\n",
    "img_std = image_train.std(axis=0, keepdims=True) + 1e-6\n",
    "\n",
    "txt_mean = caption_train.mean(axis=0, keepdims=True)\n",
    "txt_std = caption_train.std(axis=0, keepdims=True) + 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61339446-e9f4-466f-878a-76f0a0005ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_images(x):\n",
    "    return (x - img_mean) / img_std\n",
    "\n",
    "def normalize_texts(x):\n",
    "    return (x - txt_mean) / txt_std\n",
    "\n",
    "image_train = normalize_images(image_train)\n",
    "image_val   = normalize_images(image_val)\n",
    "\n",
    "caption_train = normalize_texts(caption_train)\n",
    "caption_val   = normalize_texts(caption_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "497a8934-4ca5-405d-a1a3-ff00df0c4838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Dataset that returns paired (image_feat, caption_feat) for each caption\n",
    "class CaptionImagePairedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Iterates over captions. For index i, returns:\n",
    "      caption_features[i], image_features[ caption_to_image_idx[i] ]\n",
    "    \"\"\"\n",
    "    def __init__(self, caption_feats, image_feats, caption_to_image_idx):\n",
    "        assert len(caption_feats) == len(caption_to_image_idx)\n",
    "        self.caption_feats = caption_feats.astype(np.float32)\n",
    "        self.image_feats = image_feats.astype(np.float32)\n",
    "        self.cap2img = caption_to_image_idx.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.caption_feats)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cap = self.caption_feats[idx]\n",
    "        img = self.image_feats[self.cap2img[idx]]\n",
    "        return {\"image\": torch.from_numpy(img), \"caption\": torch.from_numpy(cap)}\n",
    "\n",
    "train_dataset = CaptionImagePairedDataset(caption_train, image_train, cap2img_train)\n",
    "val_dataset   = CaptionImagePairedDataset(caption_val, image_val, cap2img_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093368f1-d9c7-4659-87b6-04ac4a111553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Model: two autoencoders with shared latent dimension\n",
    "class ImageAE(nn.Module):\n",
    "    def __init__(self, input_dim=2048, hidden_dim=1024, latent_dim=512):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        return z, recon\n",
    "\n",
    "class TextAE(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=512, latent_dim=512):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        return z, recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d769b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrastive loss function\n",
    "def contrastive_loss(z_img, z_txt, temperature=0.07):\n",
    "    # normalize both embeddings\n",
    "    z_img_norm = F.normalize(z_img, dim=1)\n",
    "    z_txt_norm = F.normalize(z_txt, dim=1)\n",
    "\n",
    "    # compute similarity matrix (batch_size x batch_size )\n",
    "    logits = torch.matmul(z_img_norm, z_txt_norm.T) / temperature\n",
    "\n",
    "    # create labels\n",
    "    labels = torch.arange(z_img.size(0)).to(z_img.device)\n",
    "\n",
    "    # compute cross-entropy for image to text direction\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "\n",
    "    # compute cross-entropy for text-to-image direction\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "    \n",
    "    # average the two losses\n",
    "    return (loss_i2t + loss_t2i) /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23ba263b-8371-4757-ae68-1212544e9782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate models\n",
    "img_ae = ImageAE(\n",
    "    input_dim=config[\"img_input_dim\"],\n",
    "    hidden_dim=config[\"img_hidden\"],\n",
    "    latent_dim=config[\"latent_dim\"]\n",
    ").to(device)\n",
    "\n",
    "txt_ae = TextAE(\n",
    "    input_dim=config[\"txt_input_dim\"],\n",
    "    hidden_dim=config[\"txt_hidden\"],\n",
    "    latent_dim=config[\"latent_dim\"]\n",
    ").to(device)\n",
    "\n",
    "# 6) Losses and optimizer\n",
    "recon_loss_fn = nn.MSELoss()    # reconstruction for both\n",
    "# align_loss_fn = nn.MSELoss()    # align latents\n",
    "\n",
    "params = list(img_ae.parameters()) + list(txt_ae.parameters())\n",
    "optimizer = Adam(params, lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc772d0",
   "metadata": {},
   "source": [
    "6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2837db3-a3a4-4e93-b0b6-2c3025543674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Training / validation loop\n",
    "def run_epoch(loader, training=True):\n",
    "    if training:\n",
    "        img_ae.train(); txt_ae.train()\n",
    "    else:\n",
    "        img_ae.eval(); txt_ae.eval()\n",
    "\n",
    "    total_recon_img = 0.0\n",
    "    total_recon_txt = 0.0\n",
    "    total_align = 0.0\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"train\" if training else \"val\")\n",
    "    with torch.set_grad_enabled(training):\n",
    "        for batch in pbar:\n",
    "            imgs = batch[\"image\"].to(device)    # shape (B, img_dim)\n",
    "            caps = batch[\"caption\"].to(device)  # shape (B, txt_dim)\n",
    "            batch_size = imgs.shape[0]\n",
    "\n",
    "            # forward\n",
    "            z_img, img_recon = img_ae(imgs)\n",
    "            z_txt, txt_recon = txt_ae(caps)\n",
    "\n",
    "            # losses\n",
    "            L_img = recon_loss_fn(img_recon, imgs)\n",
    "            L_txt = recon_loss_fn(txt_recon, caps)\n",
    "            # L_align = align_loss_fn(z_img, z_txt)\n",
    "            L_align = contrastive_loss(z_img, z_txt, temperature=0.7)\n",
    "\n",
    "            loss = L_img + L_txt + config[\"lambda_align\"] * L_align\n",
    "\n",
    "            if training:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_recon_img += L_img.item() * batch_size\n",
    "            total_recon_txt += L_txt.item() * batch_size\n",
    "            total_align += L_align.item() * batch_size\n",
    "            total_loss += loss.item() * batch_size\n",
    "            n_samples += batch_size\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{total_loss / n_samples:.4f}\",\n",
    "                \"Limg\": f\"{total_recon_img / n_samples:.4f}\",\n",
    "                \"Ltxt\": f\"{total_recon_txt / n_samples:.4f}\",\n",
    "                \"Lalign\": f\"{total_align / n_samples:.4f}\"\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / n_samples,\n",
    "        \"Limg\": total_recon_img / n_samples,\n",
    "        \"Ltxt\": total_recon_txt / n_samples,\n",
    "        \"Lalign\": total_align / n_samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc69c05c-7a7c-49bd-b742-eccc591b52a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:20<00:00, 11.06it/s, loss=4.8599, Limg=0.4567, Ltxt=0.3779, Lalign=4.0253]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 52.04it/s, loss=4.6222, Limg=0.3231, Ltxt=0.2561, Lalign=4.0430]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.8599 | Val loss: 4.6222\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 2/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:20<00:00, 10.97it/s, loss=4.3459, Limg=0.2331, Ltxt=0.2241, Lalign=3.8888]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 60.32it/s, loss=4.5043, Limg=0.2619, Ltxt=0.2119, Lalign=4.0305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.3459 | Val loss: 4.5043\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 3/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.78it/s, loss=4.2180, Limg=0.1850, Ltxt=0.1899, Lalign=3.8431]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 51.84it/s, loss=4.4421, Limg=0.2350, Ltxt=0.1860, Lalign=4.0211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.2180 | Val loss: 4.4421\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 4/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.90it/s, loss=4.1479, Limg=0.1612, Ltxt=0.1713, Lalign=3.8155]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 58.59it/s, loss=4.4190, Limg=0.2269, Ltxt=0.1724, Lalign=4.0197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.1479 | Val loss: 4.4190\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 5/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 12.07it/s, loss=4.1051, Limg=0.1481, Ltxt=0.1607, Lalign=3.7963]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 60.46it/s, loss=4.4039, Limg=0.2196, Ltxt=0.1662, Lalign=4.0181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.1051 | Val loss: 4.4039\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 6/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.89it/s, loss=4.0716, Limg=0.1383, Ltxt=0.1513, Lalign=3.7819]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 60.07it/s, loss=4.3838, Limg=0.2150, Ltxt=0.1548, Lalign=4.0140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.0716 | Val loss: 4.3838\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 7/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 12.01it/s, loss=4.0492, Limg=0.1318, Ltxt=0.1453, Lalign=3.7722]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 61.16it/s, loss=4.3830, Limg=0.2154, Ltxt=0.1537, Lalign=4.0139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.0492 | Val loss: 4.3830\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 8/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:19<00:00, 11.68it/s, loss=4.0290, Limg=0.1271, Ltxt=0.1396, Lalign=3.7622]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 57.36it/s, loss=4.3710, Limg=0.2072, Ltxt=0.1456, Lalign=4.0181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.0290 | Val loss: 4.3710\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 9/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.85it/s, loss=4.0113, Limg=0.1220, Ltxt=0.1355, Lalign=3.7538]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 62.66it/s, loss=4.3649, Limg=0.2077, Ltxt=0.1426, Lalign=4.0146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.0113 | Val loss: 4.3649\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 10/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 12.09it/s, loss=3.9992, Limg=0.1191, Ltxt=0.1315, Lalign=3.7485]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 60.97it/s, loss=4.3698, Limg=0.2115, Ltxt=0.1425, Lalign=4.0159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.9992 | Val loss: 4.3698\n",
      "\n",
      "=== Epoch 11/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 12.09it/s, loss=3.9872, Limg=0.1170, Ltxt=0.1273, Lalign=3.7429]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 58.54it/s, loss=4.3577, Limg=0.2080, Ltxt=0.1356, Lalign=4.0141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.9872 | Val loss: 4.3577\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 12/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.80it/s, loss=3.9767, Limg=0.1140, Ltxt=0.1254, Lalign=3.7373]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 52.62it/s, loss=4.3507, Limg=0.2033, Ltxt=0.1290, Lalign=4.0184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.9767 | Val loss: 4.3507\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 13/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 12.03it/s, loss=3.9642, Limg=0.1110, Ltxt=0.1215, Lalign=3.7318]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 54.66it/s, loss=4.3518, Limg=0.2028, Ltxt=0.1339, Lalign=4.0152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.9642 | Val loss: 4.3518\n",
      "\n",
      "=== Epoch 14/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.97it/s, loss=3.9581, Limg=0.1100, Ltxt=0.1195, Lalign=3.7286]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 58.58it/s, loss=4.3530, Limg=0.2041, Ltxt=0.1291, Lalign=4.0198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.9581 | Val loss: 4.3530\n",
      "\n",
      "=== Epoch 15/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:20<00:00, 11.01it/s, loss=3.9493, Limg=0.1086, Ltxt=0.1164, Lalign=3.7243]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 52.72it/s, loss=4.3508, Limg=0.2014, Ltxt=0.1308, Lalign=4.0186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.9493 | Val loss: 4.3508\n",
      "\n",
      "=== Epoch 16/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:19<00:00, 11.59it/s, loss=3.9414, Limg=0.1060, Ltxt=0.1146, Lalign=3.7208]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 57.55it/s, loss=4.3467, Limg=0.2029, Ltxt=0.1221, Lalign=4.0217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.9414 | Val loss: 4.3467\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 17/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.99it/s, loss=3.9356, Limg=0.1057, Ltxt=0.1119, Lalign=3.7180]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 53.67it/s, loss=4.3500, Limg=0.2060, Ltxt=0.1210, Lalign=4.0230]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.9356 | Val loss: 4.3500\n",
      "\n",
      "=== Epoch 18/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:19<00:00, 11.66it/s, loss=3.9326, Limg=0.1050, Ltxt=0.1106, Lalign=3.7170]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 58.00it/s, loss=4.3378, Limg=0.2011, Ltxt=0.1188, Lalign=4.0180]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.9326 | Val loss: 4.3378\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 19/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 12.24it/s, loss=3.9244, Limg=0.1031, Ltxt=0.1076, Lalign=3.7138]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 59.13it/s, loss=4.3391, Limg=0.2036, Ltxt=0.1173, Lalign=4.0182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.9244 | Val loss: 4.3391\n",
      "\n",
      "=== Epoch 20/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 12.01it/s, loss=3.9194, Limg=0.1024, Ltxt=0.1066, Lalign=3.7104]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 59.49it/s, loss=4.3401, Limg=0.2048, Ltxt=0.1129, Lalign=4.0225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.9194 | Val loss: 4.3401\n",
      "\n",
      "=== Epoch 21/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.73it/s, loss=3.9142, Limg=0.1016, Ltxt=0.1039, Lalign=3.7087]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 57.38it/s, loss=4.3320, Limg=0.2037, Ltxt=0.1120, Lalign=4.0162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.9142 | Val loss: 4.3320\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 22/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.69it/s, loss=3.9088, Limg=0.1005, Ltxt=0.1020, Lalign=3.7062]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 55.40it/s, loss=4.3303, Limg=0.2013, Ltxt=0.1119, Lalign=4.0172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.9088 | Val loss: 4.3303\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 23/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.97it/s, loss=3.9044, Limg=0.0991, Ltxt=0.1006, Lalign=3.7048]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 60.32it/s, loss=4.3359, Limg=0.2025, Ltxt=0.1097, Lalign=4.0236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.9044 | Val loss: 4.3359\n",
      "\n",
      "=== Epoch 24/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 12.09it/s, loss=3.9015, Limg=0.0995, Ltxt=0.0992, Lalign=3.7028]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 52.05it/s, loss=4.3281, Limg=0.2008, Ltxt=0.1078, Lalign=4.0196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.9015 | Val loss: 4.3281\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 25/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.98it/s, loss=3.8953, Limg=0.0981, Ltxt=0.0966, Lalign=3.7006]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 58.28it/s, loss=4.3261, Limg=0.1992, Ltxt=0.1079, Lalign=4.0190]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.8953 | Val loss: 4.3261\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 26/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 12.05it/s, loss=3.8932, Limg=0.0981, Ltxt=0.0960, Lalign=3.6990]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 58.71it/s, loss=4.3216, Limg=0.2006, Ltxt=0.1048, Lalign=4.0162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.8932 | Val loss: 4.3216\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 27/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.96it/s, loss=3.8875, Limg=0.0967, Ltxt=0.0935, Lalign=3.6973]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 55.73it/s, loss=4.3244, Limg=0.2018, Ltxt=0.1044, Lalign=4.0181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.8875 | Val loss: 4.3244\n",
      "\n",
      "=== Epoch 28/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:19<00:00, 11.34it/s, loss=3.8846, Limg=0.0962, Ltxt=0.0923, Lalign=3.6961]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 54.19it/s, loss=4.3247, Limg=0.2013, Ltxt=0.1017, Lalign=4.0218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.8846 | Val loss: 4.3247\n",
      "\n",
      "=== Epoch 29/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:19<00:00, 11.67it/s, loss=3.8805, Limg=0.0952, Ltxt=0.0911, Lalign=3.6943]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 54.75it/s, loss=4.3243, Limg=0.2010, Ltxt=0.1027, Lalign=4.0206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.8805 | Val loss: 4.3243\n",
      "\n",
      "=== Epoch 30/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:20<00:00, 10.99it/s, loss=3.8762, Limg=0.0950, Ltxt=0.0896, Lalign=3.6916]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 57.81it/s, loss=4.3162, Limg=0.2034, Ltxt=0.0967, Lalign=4.0161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.8762 | Val loss: 4.3162\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 31/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.77it/s, loss=3.8740, Limg=0.0951, Ltxt=0.0874, Lalign=3.6915]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 60.36it/s, loss=4.3208, Limg=0.1991, Ltxt=0.0968, Lalign=4.0249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.8740 | Val loss: 4.3208\n",
      "\n",
      "=== Epoch 32/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:19<00:00, 11.68it/s, loss=3.8693, Limg=0.0936, Ltxt=0.0864, Lalign=3.6893]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 57.48it/s, loss=4.3238, Limg=0.2023, Ltxt=0.0970, Lalign=4.0244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.8693 | Val loss: 4.3238\n",
      "\n",
      "=== Epoch 33/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:19<00:00, 11.66it/s, loss=3.8678, Limg=0.0947, Ltxt=0.0852, Lalign=3.6879]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 58.93it/s, loss=4.3179, Limg=0.2032, Ltxt=0.0925, Lalign=4.0222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.8678 | Val loss: 4.3179\n",
      "\n",
      "=== Epoch 34/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.99it/s, loss=3.8654, Limg=0.0936, Ltxt=0.0835, Lalign=3.6883]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 55.00it/s, loss=4.3167, Limg=0.2020, Ltxt=0.0925, Lalign=4.0222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.8654 | Val loss: 4.3167\n",
      "\n",
      "=== Epoch 35/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.94it/s, loss=3.8628, Limg=0.0933, Ltxt=0.0829, Lalign=3.6865]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 58.60it/s, loss=4.3168, Limg=0.2015, Ltxt=0.0918, Lalign=4.0235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.8628 | Val loss: 4.3168\n",
      "\n",
      "=== Epoch 36/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 12.02it/s, loss=3.8599, Limg=0.0929, Ltxt=0.0815, Lalign=3.6855]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 58.79it/s, loss=4.3125, Limg=0.1996, Ltxt=0.0875, Lalign=4.0255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.8599 | Val loss: 4.3125\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 37/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.98it/s, loss=3.8559, Limg=0.0923, Ltxt=0.0799, Lalign=3.6837]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 59.67it/s, loss=4.3139, Limg=0.2007, Ltxt=0.0894, Lalign=4.0238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.8559 | Val loss: 4.3139\n",
      "\n",
      "=== Epoch 38/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.91it/s, loss=3.8536, Limg=0.0924, Ltxt=0.0789, Lalign=3.6823]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 60.26it/s, loss=4.3176, Limg=0.2019, Ltxt=0.0920, Lalign=4.0237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.8536 | Val loss: 4.3176\n",
      "\n",
      "=== Epoch 39/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.90it/s, loss=3.8513, Limg=0.0911, Ltxt=0.0785, Lalign=3.6817]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 55.38it/s, loss=4.3109, Limg=0.1992, Ltxt=0.0868, Lalign=4.0248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.8513 | Val loss: 4.3109\n",
      "Saved best checkpoint.\n",
      "\n",
      "=== Epoch 40/40 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:18<00:00, 11.90it/s, loss=3.8497, Limg=0.0912, Ltxt=0.0769, Lalign=3.6816]\n",
      "val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:00<00:00, 60.50it/s, loss=4.3101, Limg=0.2027, Ltxt=0.0871, Lalign=4.0203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.8497 | Val loss: 4.3101\n",
      "Saved best checkpoint.\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, config[\"epochs\"] + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{config['epochs']} ===\")\n",
    "    train_metrics = run_epoch(train_loader, training=True)\n",
    "    val_metrics = run_epoch(val_loader, training=False)\n",
    "\n",
    "    print(f\"Train loss: {train_metrics['loss']:.4f} | Val loss: {val_metrics['loss']:.4f}\")\n",
    "\n",
    "    # Save checkpoint (every epoch)\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch,\n",
    "        \"img_state\": img_ae.state_dict(),\n",
    "        \"txt_state\": txt_ae.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"train_metrics\": train_metrics,\n",
    "        \"val_metrics\": val_metrics,\n",
    "        \"config\": config\n",
    "    }\n",
    "    ckpt_path = os.path.join(config[\"checkpoint_dir\"], f\"corr_ae_epoch{epoch}.pt\")\n",
    "    torch.save(ckpt, ckpt_path)\n",
    "\n",
    "    # Keep best\n",
    "    if val_metrics[\"loss\"] < best_val_loss:\n",
    "        best_val_loss = val_metrics[\"loss\"]\n",
    "        torch.save(ckpt, os.path.join(config[\"checkpoint_dir\"], \"corr_ae_best.pt\"))\n",
    "        print(\"Saved best checkpoint.\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b00a83",
   "metadata": {},
   "source": [
    "7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d6522d1f-3127-44bc-b34f-1398e3938c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best checkpoint from ./corr_ae_checkpoints_contrastive/corr_ae_best.pt (epoch 40)\n"
     ]
    }
   ],
   "source": [
    "#Now time to evaluate on the validation set\n",
    "#Probably don't need to reload the model, but I'm going to include the code again in case we break this up into more managable files\n",
    "# --- Load best checkpoint ---\n",
    "best_ckpt_path = os.path.join(config[\"checkpoint_dir\"], \"corr_ae_best.pt\")\n",
    "ckpt = torch.load(best_ckpt_path, map_location=device)\n",
    "\n",
    "img_ae.load_state_dict(ckpt[\"img_state\"])\n",
    "txt_ae.load_state_dict(ckpt[\"txt_state\"])\n",
    "img_ae.eval(); txt_ae.eval()\n",
    "\n",
    "print(f\"Loaded best checkpoint from {best_ckpt_path} (epoch {ckpt['epoch']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ded49cc-65d2-4e10-8403-155c03e9eb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded latent shapes: (1214, 512) (6070, 512)\n"
     ]
    }
   ],
   "source": [
    "# Encode into latent space\n",
    "with torch.no_grad():\n",
    "    # Encode images\n",
    "    Z_imgs = []\n",
    "    for i in range(0, image_val.shape[0], 256):\n",
    "        batch = torch.from_numpy(image_val[i:i+256]).float().to(device)\n",
    "        z, _ = img_ae(batch)\n",
    "        Z_imgs.append(z.cpu().numpy())\n",
    "    Z_imgs = np.concatenate(Z_imgs, axis=0)   # shape (N_images, latent_dim)\n",
    "\n",
    "    # Encode captions\n",
    "    Z_caps = []\n",
    "    for i in range(0, caption_val.shape[0], 256):\n",
    "        batch = torch.from_numpy(caption_val[i:i+256]).float().to(device)\n",
    "        z, _ = txt_ae(batch)\n",
    "        Z_caps.append(z.cpu().numpy())\n",
    "    Z_caps = np.concatenate(Z_caps, axis=0)   # shape (N_captions, latent_dim)\n",
    "\n",
    "print(\"Encoded latent shapes:\", Z_imgs.shape, Z_caps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2564e05-cb4d-4429-be2b-871e89bc3722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1: 0.0465\n",
      "Recall@5: 0.1588\n",
      "Recall@10: 0.2631\n",
      "MedianRank: 30.0000\n"
     ]
    }
   ],
   "source": [
    "#Use Recall@1/5/10 to evaluate hyperparameter performance\n",
    "#Note that we are using cosine similarity\n",
    "#Should we consider using L2 metric instead? Does this even make sense?\n",
    "def retrieval_metrics(Z_caps, Z_imgs, caption_to_image_idx):\n",
    "    sims = cosine_similarity(Z_caps, Z_imgs)  # (num_caps, num_imgs)\n",
    "    ranks = []\n",
    "    for i, true_img_idx in enumerate(caption_to_image_idx):\n",
    "        sim_scores = sims[i]\n",
    "        sorted_indices = np.argsort(-sim_scores)  # descending\n",
    "        rank = np.where(sorted_indices == true_img_idx)[0][0] + 1\n",
    "        ranks.append(rank)\n",
    "\n",
    "    ranks = np.array(ranks)\n",
    "    recall_at_1  = np.mean(ranks <= 1)\n",
    "    recall_at_5  = np.mean(ranks <= 5)\n",
    "    recall_at_10 = np.mean(ranks <= 10)\n",
    "    med_rank = np.median(ranks)\n",
    "\n",
    "    return {\n",
    "        \"Recall@1\": recall_at_1,\n",
    "        \"Recall@5\": recall_at_5,\n",
    "        \"Recall@10\": recall_at_10,\n",
    "        \"MedianRank\": med_rank\n",
    "    }\n",
    "\n",
    "metrics_val = retrieval_metrics(Z_caps, Z_imgs, cap2img_val)\n",
    "for k, v in metrics_val.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e125ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded latent shapes: (1214, 512) (6070, 512)\n",
      "Recall@1: 0.0007\n",
      "Recall@5: 0.0035\n",
      "Recall@10: 0.0086\n",
      "MedianRank: 600.0000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Encode into latent space\n",
    "with torch.no_grad():\n",
    "    # Encode images\n",
    "    Z_imgs = []\n",
    "    for i in range(0, image_test.shape[0], 256):\n",
    "        batch = torch.from_numpy(image_test[i:i+256]).float().to(device)\n",
    "        z, _ = img_ae(batch)\n",
    "        Z_imgs.append(z.cpu().numpy())\n",
    "    Z_imgs = np.concatenate(Z_imgs, axis=0)   # shape (N_images, latent_dim)\n",
    "\n",
    "    # Encode captions\n",
    "    Z_caps = []\n",
    "    for i in range(0, caption_test.shape[0], 256):\n",
    "        batch = torch.from_numpy(caption_test[i:i+256]).float().to(device)\n",
    "        z, _ = txt_ae(batch)\n",
    "        Z_caps.append(z.cpu().numpy())\n",
    "    Z_caps = np.concatenate(Z_caps, axis=0)   # shape (N_captions, latent_dim)\n",
    "\n",
    "print(\"Encoded latent shapes:\", Z_imgs.shape, Z_caps.shape)\n",
    "\n",
    "def retrieval_metrics(Z_caps, Z_imgs, caption_to_image_idx):\n",
    "    sims = cosine_similarity(Z_caps, Z_imgs)  # (num_caps, num_imgs)\n",
    "    ranks = []\n",
    "    for i, true_img_idx in enumerate(caption_to_image_idx):\n",
    "        sim_scores = sims[i]\n",
    "        sorted_indices = np.argsort(-sim_scores)  # descending\n",
    "        rank = np.where(sorted_indices == true_img_idx)[0][0] + 1\n",
    "        ranks.append(rank)\n",
    "\n",
    "    ranks = np.array(ranks)\n",
    "    recall_at_1  = np.mean(ranks <= 1)\n",
    "    recall_at_5  = np.mean(ranks <= 5)\n",
    "    recall_at_10 = np.mean(ranks <= 10)\n",
    "    med_rank = np.median(ranks)\n",
    "\n",
    "    return {\n",
    "        \"Recall@1\": recall_at_1,\n",
    "        \"Recall@5\": recall_at_5,\n",
    "        \"Recall@10\": recall_at_10,\n",
    "        \"MedianRank\": med_rank\n",
    "    }\n",
    "\n",
    "metrics_val = retrieval_metrics(Z_caps, Z_imgs, cap2img_val)\n",
    "for k, v in metrics_val.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7z27rtos1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization: get actual caption texts and image filenames for validation set\n",
    "# Extract the validation caption texts from the original dataframe\n",
    "val_caption_texts = df[\"caption\"].values[val_mask]\n",
    "\n",
    "# Extract the validation image filenames\n",
    "val_image_names = np.array([image_names[i] for i in val_idx])\n",
    "\n",
    "print(f\"Loaded {len(val_caption_texts)} caption texts and {len(val_image_names)} image filenames for validation\")\n",
    "print(f\"Example caption: {val_caption_texts[0]}\")\n",
    "print(f\"Example image: {val_image_names[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e75e4-ba15-4a36-aed3-86924673b338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick visualization of what images are retrieved by what caption:\n",
    "image_dir = \"/Users/sfowler14/Downloads/archive/Images\"  # Update this if your images are elsewhere\n",
    "\n",
    "def show_top_images_for_caption(caption_idx, top_k=5):\n",
    "    \"\"\"\n",
    "    Show top-k retrieved validation images for a given caption index.\n",
    "    Also displays the true image for comparison.\n",
    "    \"\"\"\n",
    "    # Get the embedding for this caption\n",
    "    caption_embedding = Z_caps[caption_idx].reshape(1, -1)\n",
    "    sims = cosine_similarity(caption_embedding, Z_imgs)[0]\n",
    "    top_img_indices = np.argsort(-sims)[:top_k]\n",
    "\n",
    "    # Print caption text\n",
    "    print(f\"\\nCAPTION: {val_caption_texts[caption_idx]}\")\n",
    "    true_img_idx = cap2img_val[caption_idx]\n",
    "    print(f\"TRUE IMAGE: {val_image_names[true_img_idx]} (index {true_img_idx})\")\n",
    "    \n",
    "    # Display: true image + top k retrieved images\n",
    "    plt.figure(figsize=(18, 4))\n",
    "    \n",
    "    # Show the true image first\n",
    "    true_img_name = val_image_names[true_img_idx]\n",
    "    true_img_path = os.path.join(image_dir, true_img_name)\n",
    "    try:\n",
    "        true_img = Image.open(true_img_path)\n",
    "        plt.subplot(1, top_k + 1, 1)\n",
    "        plt.imshow(true_img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"TRUE IMAGE\", fontweight='bold', color='green')\n",
    "    except Exception as e:\n",
    "        print(f\"Could not open true image {true_img_path}: {e}\")\n",
    "    \n",
    "    # Show retrieved images\n",
    "    for i, img_idx in enumerate(top_img_indices):\n",
    "        img_name = val_image_names[img_idx]\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not open {img_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        plt.subplot(1, top_k + 1, i + 2)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Highlight if this retrieved image matches the true image\n",
    "        if img_idx == true_img_idx:\n",
    "            plt.title(f\"Rank {i+1} \u2713\", fontweight='bold', color='green')\n",
    "        else:\n",
    "            plt.title(f\"Rank {i+1}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eade03-1612-4ab1-9d94-aab8ee44eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "for i in random.sample(range(len(caption_val)), 3):\n",
    "    show_top_images_for_caption(i, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa06d10-0491-4943-bc96-fb94c2575ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}