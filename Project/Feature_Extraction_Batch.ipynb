{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa8c8488",
   "metadata": {},
   "source": [
    "1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d92e9d-41e9-4e9e-ab17-06d210dcb4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sfowler14/miniconda3/envs/DLD/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84583e74-3167-497b-84a2-05e99c1902e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your zip file\n",
    "zip_path = os.path.expanduser('~/Downloads/archive(1).zip')\n",
    "extract_dir = './flickr8k_data'\n",
    "\n",
    "# Extract only if not already done\n",
    "if not os.path.exists(extract_dir):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(\"Extracted to:\", extract_dir)\n",
    "print(\"Contents:\", os.listdir(extract_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69f37da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted to: /Users/sfowler14/Downloads/archive\n",
      "Contents: ['captions.txt', 'Images']\n"
     ]
    }
   ],
   "source": [
    "extract_dir = '/Users/sfowler14/Downloads/archive'\n",
    "print(\"Extracted to:\", extract_dir)\n",
    "print(\"Contents:\", os.listdir(extract_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c23613b-4b9f-4cb5-8ad7-18de92763088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       image  \\\n",
      "0  1000268201_693b08cb0e.jpg   \n",
      "1  1000268201_693b08cb0e.jpg   \n",
      "2  1000268201_693b08cb0e.jpg   \n",
      "3  1000268201_693b08cb0e.jpg   \n",
      "4  1000268201_693b08cb0e.jpg   \n",
      "\n",
      "                                             caption  \n",
      "0  A child in a pink dress is climbing up a set o...  \n",
      "1              A girl going into a wooden building .  \n",
      "2   A little girl climbing into a wooden playhouse .  \n",
      "3  A little girl climbing the stairs to her playh...  \n",
      "4  A little girl in a pink dress going into a woo...  \n",
      "\n",
      "Number of unique images: 8091\n"
     ]
    }
   ],
   "source": [
    "captions_path = os.path.join(extract_dir, 'captions.txt')\n",
    "df = pd.read_csv(captions_path)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"\\nNumber of unique images: {df['image'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbc45e",
   "metadata": {},
   "source": [
    "2. Image Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b5f786-9708-48bc-8f4e-74f8a33ce4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Image transform (standard ImageNet normalization) ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Dataset class for images ---\n",
    "class FlickrImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_filenames, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_filenames = list(image_filenames)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_filenames[idx]\n",
    "        path = os.path.join(self.image_dir, img_name)\n",
    "        # wrap in try-except to catch corrupt images\n",
    "        try:\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            # if image fails to open, create a black image instead and log\n",
    "            print(f\"Failed to open {path}: {e}\")\n",
    "            image = Image.new('RGB', (224,224))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4241e3a5-c3cf-4a44-aac5-41b16073cf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sfowler14/miniconda3/envs/DLD/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sfowler14/miniconda3/envs/DLD/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# --- Model setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*(list(resnet.children())[:-1]))  # remove FC layer\n",
    "resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "# --- Create dataset and dataloader ---\n",
    "image_dir = os.path.join(extract_dir, \"Images\")\n",
    "unique_images = df[\"image\"].unique()\n",
    "image_dataset = FlickrImageDataset(image_dir, unique_images, transform)\n",
    "image_loader = DataLoader(image_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e945bc06-f5ba-4ced-a3c4-d00b25ab4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# test model forward\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*(list(resnet.children())[:-1])).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085dd006-09e8-4cbe-95de-4243928ca6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames = sorted(os.listdir(image_dir))   # better to use df['image'].unique() if you want same order\n",
    "dataset = FlickrImageDataset(image_dir, image_filenames, transform)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)  # num_workers=0 is safest\n",
    "\n",
    "image_features_list = []\n",
    "image_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2532870d-6a66-4a27-9b0e-240facf3136b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting image features: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 253/253 [11:36<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. image_features shape: (8091, 2048)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for imgs, names in tqdm(loader, desc=\"Extracting image features\"):\n",
    "        imgs = imgs.to(device)\n",
    "        feats = resnet(imgs)               # (B, 2048, 1, 1)\n",
    "        feats = feats.view(feats.size(0), -1)  # (B, 2048)\n",
    "        image_features_list.append(feats.cpu())\n",
    "        image_names.extend(names)\n",
    "\n",
    "image_features = torch.cat(image_features_list, dim=0).numpy()\n",
    "print(\"Done. image_features shape:\", image_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b3da1",
   "metadata": {},
   "source": [
    "3. Caption Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e8c92bf-4053-411d-8ea1-387e1bc80261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 14:25:55.733525: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# --- Device and model setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert.to(device)\n",
    "bert.eval()\n",
    "\n",
    "# --- Dataset for captions ---\n",
    "class FlickrCaptionDataset(Dataset):\n",
    "    def __init__(self, captions):\n",
    "        self.captions = captions\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.captions[idx]\n",
    "        # Return plain text; we\u2019ll tokenize in collate_fn for batching\n",
    "        return text\n",
    "\n",
    "# --- Custom collate_fn to batch tokenize ---\n",
    "def collate_fn(batch_texts):\n",
    "    return tokenizer(batch_texts, return_tensors='pt',\n",
    "                     truncation=True, padding=True, max_length=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b7a232-4bda-431f-b763-24ad6e5145b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create dataset and dataloader ---\n",
    "caption_dataset = FlickrCaptionDataset(df[\"caption\"].tolist())\n",
    "caption_loader = DataLoader(\n",
    "    caption_dataset,\n",
    "    batch_size=32,         \n",
    "    shuffle=False,\n",
    "    num_workers=0,         \n",
    "    collate_fn=collate_fn  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dea1e630-d628-4cef-b566-be056a36f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting caption features: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1265/1265 [14:39<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption features shape: (40455, 768)\n"
     ]
    }
   ],
   "source": [
    "# --- Extract features ---\n",
    "caption_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(caption_loader, desc=\"Extracting caption features\"):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = bert(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]  # (B, 768)\n",
    "        caption_features.append(cls_embeddings.cpu())\n",
    "\n",
    "caption_features = torch.cat(caption_features, dim=0).numpy()\n",
    "\n",
    "print(\"Caption features shape:\", caption_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b025dbc",
   "metadata": {},
   "source": [
    "4. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05249b54-abb8-4e47-ba59-c7b20149aa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caption_to_image_idx shape: (40455,)\n",
      "Example mapping: [('A child in a pink dress is climbing up a set of stairs in an entry way .', 0), ('A girl going into a wooden building .', 0), ('A little girl climbing into a wooden playhouse .', 0)]\n"
     ]
    }
   ],
   "source": [
    "# Map image filename to its index\n",
    "image_to_idx = {name: i for i, name in enumerate(image_names)}\n",
    "\n",
    "# For each caption row, find which image it corresponds to\n",
    "caption_to_image_idx = df[\"image\"].map(image_to_idx).values\n",
    "\n",
    "print(\"caption_to_image_idx shape:\", caption_to_image_idx.shape)\n",
    "print(\"Example mapping:\", list(zip(df['caption'][:3], caption_to_image_idx[:3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e74e7d1d-f1ba-40a8-9206-91cb3c47846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"flickr8k_image_features.npy\", image_features)\n",
    "np.save(\"flickr8k_caption_features.npy\", caption_features)\n",
    "np.save(\"flickr8k_caption_to_image.npy\", caption_to_image_idx)\n",
    "np.save(\"flickr8k_image_names.npy\", np.array(image_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61562bc4-f27c-4e4b-b95f-5fca476671b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on images\n",
    "n_images = len(image_names)\n",
    "indices = np.arange(n_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "577132b8-5241-475f-8be3-162740a15160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 5663, Val: 1214, Test: 1214\n"
     ]
    }
   ],
   "source": [
    "# 70/15/15 split\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.30, random_state=42, shuffle=True)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42, shuffle=True)\n",
    "print(f\"Train images: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afdcdbb8-a6fa-4155-8e4f-19c7285e7449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks for captions \n",
    "caption_to_image_idx = caption_to_image_idx.astype(int)\n",
    "\n",
    "train_mask = np.isin(caption_to_image_idx, train_idx)\n",
    "val_mask = np.isin(caption_to_image_idx, val_idx)\n",
    "test_mask = np.isin(caption_to_image_idx, test_idx)\n",
    "\n",
    "# Split image features \n",
    "image_train = image_features[train_idx]\n",
    "image_val   = image_features[val_idx]\n",
    "image_test  = image_features[test_idx]\n",
    "\n",
    "# Split captions (and keep their alignment)\n",
    "caption_train = caption_features[train_mask]\n",
    "caption_val   = caption_features[val_mask]\n",
    "caption_test  = caption_features[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "360512a8-752e-4295-bfad-00b00c4eb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link captions to local image indices within each split\n",
    "def remap_caption_indices(global_indices, split_indices):\n",
    "    \"\"\"\n",
    "    Convert global image indices in caption_to_image_idx to 0..len(split_indices)-1 within that split.\n",
    "    \"\"\"\n",
    "    mapping = {g: i for i, g in enumerate(split_indices)}\n",
    "    return np.array([mapping[i] for i in global_indices if i in mapping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d4cdda6-8225-493f-b5cd-40777cf9b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_to_train_img = remap_caption_indices(caption_to_image_idx[train_mask], train_idx)\n",
    "caption_to_val_img   = remap_caption_indices(caption_to_image_idx[val_mask], val_idx)\n",
    "caption_to_test_img  = remap_caption_indices(caption_to_image_idx[test_mask], test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3891c16-d50d-455d-bdea-4f6925c451b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split shapes:\n",
      "  Image features: (5663, 2048)\n",
      "  Caption features: (28315, 768)\n",
      "  Caption\u2192Image indices: (28315,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(\"Train split shapes:\")\n",
    "print(\"  Image features:\", image_train.shape)\n",
    "print(\"  Caption features:\", caption_train.shape)\n",
    "print(\"  Caption\u2192Image indices:\", caption_to_train_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "669c7533-fbbb-4224-988c-e07b19952562",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"train_image_features.npy\", image_train)\n",
    "np.save(\"val_image_features.npy\", image_val)\n",
    "np.save(\"test_image_features.npy\", image_test)\n",
    "\n",
    "np.save(\"train_caption_features.npy\", caption_train)\n",
    "np.save(\"val_caption_features.npy\", caption_val)\n",
    "np.save(\"test_caption_features.npy\", caption_test)\n",
    "\n",
    "np.save(\"train_caption_to_image.npy\", caption_to_train_img)\n",
    "np.save(\"val_caption_to_image.npy\", caption_to_val_img)\n",
    "np.save(\"test_caption_to_image.npy\", caption_to_test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Complete!\n\n",
    "The following .npy files have been saved:\n\n",
    "**Full dataset:**\n",
    "- `flickr8k_image_features.npy`\n",
    "- `flickr8k_caption_features.npy`\n",
    "- `flickr8k_caption_to_image.npy`\n",
    "- `flickr8k_image_names.npy`\n\n",
    "**Train/Val/Test splits:**\n",
    "- `train_image_features.npy`, `train_caption_features.npy`, `train_caption_to_image.npy`\n",
    "- `val_image_features.npy`, `val_caption_features.npy`, `val_caption_to_image.npy`\n",
    "- `test_image_features.npy`, `test_caption_features.npy`, `test_caption_to_image.npy`\n\n",
    "These files can now be loaded by architecture notebooks (e.g., Architecture_1.ipynb, Cross_Modal_Autoencoder.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}