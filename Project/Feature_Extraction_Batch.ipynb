{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa8c8488",
   "metadata": {},
   "source": [
    "1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d92e9d-41e9-4e9e-ab17-06d210dcb4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84583e74-3167-497b-84a2-05e99c1902e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your zip file\n",
    "zip_path = os.path.expanduser('~/Downloads/archive(1).zip')\n",
    "extract_dir = './flickr8k_data'\n",
    "\n",
    "# Extract only if not already done\n",
    "if not os.path.exists(extract_dir):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(\"Extracted to:\", extract_dir)\n",
    "print(\"Contents:\", os.listdir(extract_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f37da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_dir = '/Users/sfowler14/Downloads/archive'\n",
    "print(\"Extracted to:\", extract_dir)\n",
    "print(\"Contents:\", os.listdir(extract_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c23613b-4b9f-4cb5-8ad7-18de92763088",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_path = os.path.join(extract_dir, 'captions.txt')\n",
    "df = pd.read_csv(captions_path)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"\\nNumber of unique images: {df['image'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbc45e",
   "metadata": {},
   "source": [
    "2. Image Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b5f786-9708-48bc-8f4e-74f8a33ce4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Image transform (standard ImageNet normalization) ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Dataset class for images ---\n",
    "class FlickrImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_filenames, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_filenames = list(image_filenames)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_filenames[idx]\n",
    "        path = os.path.join(self.image_dir, img_name)\n",
    "        # wrap in try-except to catch corrupt images\n",
    "        try:\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            # if image fails to open, create a black image instead and log\n",
    "            print(f\"Failed to open {path}: {e}\")\n",
    "            image = Image.new('RGB', (224,224))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4241e3a5-c3cf-4a44-aac5-41b16073cf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*(list(resnet.children())[:-1]))  # remove FC layer\n",
    "resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "# --- Create dataset and dataloader ---\n",
    "image_dir = os.path.join(extract_dir, \"Images\")\n",
    "unique_images = df[\"image\"].unique()\n",
    "image_dataset = FlickrImageDataset(image_dir, unique_images, transform)\n",
    "image_loader = DataLoader(image_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945bc06-f5ba-4ced-a3c4-d00b25ab4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# test model forward\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*(list(resnet.children())[:-1])).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085dd006-09e8-4cbe-95de-4243928ca6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames = sorted(os.listdir(image_dir))   # better to use df['image'].unique() if you want same order\n",
    "dataset = FlickrImageDataset(image_dir, image_filenames, transform)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)  # num_workers=0 is safest\n",
    "\n",
    "image_features_list = []\n",
    "image_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2532870d-6a66-4a27-9b0e-240facf3136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for imgs, names in tqdm(loader, desc=\"Extracting image features\"):\n",
    "        imgs = imgs.to(device)\n",
    "        feats = resnet(imgs)               # (B, 2048, 1, 1)\n",
    "        feats = feats.view(feats.size(0), -1)  # (B, 2048)\n",
    "        image_features_list.append(feats.cpu())\n",
    "        image_names.extend(names)\n",
    "\n",
    "image_features = torch.cat(image_features_list, dim=0).numpy()\n",
    "print(\"Done. image_features shape:\", image_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b3da1",
   "metadata": {},
   "source": [
    "3. Caption Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c92bf-4053-411d-8ea1-387e1bc80261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# --- Device and model setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert.to(device)\n",
    "bert.eval()\n",
    "\n",
    "# --- Dataset for captions ---\n",
    "class FlickrCaptionDataset(Dataset):\n",
    "    def __init__(self, captions):\n",
    "        self.captions = captions\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.captions[idx]\n",
    "        # Return plain text; we’ll tokenize in collate_fn for batching\n",
    "        return text\n",
    "\n",
    "# --- Custom collate_fn to batch tokenize ---\n",
    "def collate_fn(batch_texts):\n",
    "    return tokenizer(batch_texts, return_tensors='pt',\n",
    "                     truncation=True, padding=True, max_length=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b7a232-4bda-431f-b763-24ad6e5145b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create dataset and dataloader ---\n",
    "caption_dataset = FlickrCaptionDataset(df[\"caption\"].tolist())\n",
    "caption_loader = DataLoader(\n",
    "    caption_dataset,\n",
    "    batch_size=32,         \n",
    "    shuffle=False,\n",
    "    num_workers=0,         \n",
    "    collate_fn=collate_fn  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea1e630-d628-4cef-b566-be056a36f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extract features ---\n",
    "caption_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(caption_loader, desc=\"Extracting caption features\"):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = bert(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]  # (B, 768)\n",
    "        caption_features.append(cls_embeddings.cpu())\n",
    "\n",
    "caption_features = torch.cat(caption_features, dim=0).numpy()\n",
    "\n",
    "print(\"Caption features shape:\", caption_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b025dbc",
   "metadata": {},
   "source": [
    "4. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05249b54-abb8-4e47-ba59-c7b20149aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map image filename to its index\n",
    "image_to_idx = {name: i for i, name in enumerate(image_names)}\n",
    "\n",
    "# For each caption row, find which image it corresponds to\n",
    "caption_to_image_idx = df[\"image\"].map(image_to_idx).values\n",
    "\n",
    "print(\"caption_to_image_idx shape:\", caption_to_image_idx.shape)\n",
    "print(\"Example mapping:\", list(zip(df['caption'][:3], caption_to_image_idx[:3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74e7d1d-f1ba-40a8-9206-91cb3c47846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"flickr8k_image_features.npy\", image_features)\n",
    "np.save(\"flickr8k_caption_features.npy\", caption_features)\n",
    "np.save(\"flickr8k_caption_to_image.npy\", caption_to_image_idx)\n",
    "np.save(\"flickr8k_image_names.npy\", np.array(image_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61562bc4-f27c-4e4b-b95f-5fca476671b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on images\n",
    "n_images = len(image_names)\n",
    "indices = np.arange(n_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577132b8-5241-475f-8be3-162740a15160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70/15/15 split\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.30, random_state=42, shuffle=True)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42, shuffle=True)\n",
    "print(f\"Train images: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdcdbb8-a6fa-4155-8e4f-19c7285e7449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks for captions \n",
    "caption_to_image_idx = caption_to_image_idx.astype(int)\n",
    "\n",
    "train_mask = np.isin(caption_to_image_idx, train_idx)\n",
    "val_mask = np.isin(caption_to_image_idx, val_idx)\n",
    "test_mask = np.isin(caption_to_image_idx, test_idx)\n",
    "\n",
    "# Split image features \n",
    "image_train = image_features[train_idx]\n",
    "image_val   = image_features[val_idx]\n",
    "image_test  = image_features[test_idx]\n",
    "\n",
    "# Split captions (and keep their alignment)\n",
    "caption_train = caption_features[train_mask]\n",
    "caption_val   = caption_features[val_mask]\n",
    "caption_test  = caption_features[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360512a8-752e-4295-bfad-00b00c4eb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link captions to local image indices within each split\n",
    "def remap_caption_indices(global_indices, split_indices):\n",
    "    \"\"\"\n",
    "    Convert global image indices in caption_to_image_idx to 0..len(split_indices)-1 within that split.\n",
    "    \"\"\"\n",
    "    mapping = {g: i for i, g in enumerate(split_indices)}\n",
    "    return np.array([mapping[i] for i in global_indices if i in mapping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4cdda6-8225-493f-b5cd-40777cf9b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_to_train_img = remap_caption_indices(caption_to_image_idx[train_mask], train_idx)\n",
    "caption_to_val_img   = remap_caption_indices(caption_to_image_idx[val_mask], val_idx)\n",
    "caption_to_test_img  = remap_caption_indices(caption_to_image_idx[test_mask], test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3891c16-d50d-455d-bdea-4f6925c451b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(\"Train split shapes:\")\n",
    "print(\"  Image features:\", image_train.shape)\n",
    "print(\"  Caption features:\", caption_train.shape)\n",
    "print(\"  Caption→Image indices:\", caption_to_train_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c7533-fbbb-4224-988c-e07b19952562",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"train_image_features.npy\", image_train)\n",
    "np.save(\"val_image_features.npy\", image_val)\n",
    "np.save(\"test_image_features.npy\", image_test)\n",
    "\n",
    "np.save(\"train_caption_features.npy\", caption_train)\n",
    "np.save(\"val_caption_features.npy\", caption_val)\n",
    "np.save(\"test_caption_features.npy\", caption_test)\n",
    "\n",
    "np.save(\"train_caption_to_image.npy\", caption_to_train_img)\n",
    "np.save(\"val_caption_to_image.npy\", caption_to_val_img)\n",
    "np.save(\"test_caption_to_image.npy\", caption_to_test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Complete!\n",
    "\n",
    "The following .npy files have been saved:\n",
    "\n",
    "**Full dataset:**\n",
    "- `flickr8k_image_features.npy`\n",
    "- `flickr8k_caption_features.npy`\n",
    "- `flickr8k_caption_to_image.npy`\n",
    "- `flickr8k_image_names.npy`\n",
    "\n",
    "**Train/Val/Test splits:**\n",
    "- `train_image_features.npy`, `train_caption_features.npy`, `train_caption_to_image.npy`\n",
    "- `val_image_features.npy`, `val_caption_features.npy`, `val_caption_to_image.npy`\n",
    "- `test_image_features.npy`, `test_caption_features.npy`, `test_caption_to_image.npy`\n",
    "\n",
    "These files can now be loaded by architecture notebooks (e.g., Corr_AE.ipynb, Cross_Modal_Autoencoder.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
