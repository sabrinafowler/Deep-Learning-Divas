# Deep-Learning-Divas
CSCE 479/879 group work for Sabrina Fowler, Grace Hecke, Abby Veiman, Derek DeBlieck

## Virtual Environment
In order to set up a conda virtual environment using the requirements.txt file, Python version 3.10 should be used.

## Homework 1: Connected Architectures and Fashion MNIST
The assignment for Homework 1 was to design and implement at least 2 architectures for a classification task on the Fashion MNIST dataset. In particular, to load the training data from tensorflow-datasets, partition it into separate training and validation sets, use these to train and evaluate your models, and report the results. 
Model specifications were:
- For each of architecture, use Adam to optimize the parameters 
- For each training run,  use at least two sets of hyperparameters
- Choose at least one regularizer and evaluate system performance with and without it

__Deliverables:__ 
1. A single, well written report (in pdf format) discussing results. The pdf should include the experimental setup, results, and conclusions.
2. Three program files, with the following names:
    - main.py: Code that runs the main loop of training the TensorFlow models
    - model.py: TensorFlow code that defines the network
    - util.py: Helper functions (e.g., for loading the data, small repetitive functions)

## Homework 2: Convolutional Architectures and CIFAR-100
The assignment for Homework 2 was to apply convolutional neural networks to the problem of image classification from the CIFAR-100 dataset. In particular, to designand implement at least two convolutional architectures which satisfy the following specifications:

- Each architecture must use at least two convolutional+pooling layers and at least one connected layer, followed by softmax for the output layer
- Measure loss with cross-entropy
- For each architecture, choose an optimizer
- For each training run use at least two sets of hyperparameters
- Choose a regularizer, in addition to early stopping with a minimum patience hyperparameter value of 5

__Deliverables:__
1. A single, well written report (in pdf format) discussing results. The pdf should include the experimental setup, results, and conclusions.
2. Three program files, with the following names:
    - main.py: Code that runs the main loop of training the TensorFlow models
    - model.py: TensorFlow code that defines the network
    - util.py: Helper functions (e.g., for loading the data, small repetitive functions)

## Project Notes
### Notebooks:
- `Feature_Extraction_Batch(1).ipynb`: Derek's original feature extraction and first pass at CorrAE
- `Feature_Extraction_Batch.ipynb`: The first half of Derek's notebook, just does the feature extraction
- `Corr_AE.ipynb`: Derek's CorrAE architecture, with additional optional code implementing a contrastive loss function (loads `.npy` feature files generated by `Feature_Extraction_Batch.ipynb`)
- `Cross_Modal_Autoencoder.ipynb`: Sabrina's cross-modal architecture, separating encoders and decoders for each modaltiy (loads `.npy` feature files generated by `Feature_Extraction_Batch.ipynb`)

### Summary of early November experiments:

__CorrAE with MSE alignment loss__

best model at epoch 39

Validation Set Metrics:
- Recall@1: 0.0507
- Recall@5: 0.1641
- Recall@10: 0.2521
- MedianRank: 38.0000

Test Set Metrics:
- Recall@1: 0.0521
- Recall@5: 0.1667
- Recall@10: 0.2486
- MedianRank: 42.0000

__CorrAE with contrastive alignment loss__

best model at epoch 40

Validation Set Metrics:
- Recall@1: 0.0465
- Recall@5: 0.1588
- Recall@10: 0.2631
- MedianRank: 30.0000

Test Set Metrics:
- Recall@1: 0.0504
- Recall@5: 0.1741
- Recall@10: 0.2768
- MedianRank: 30.0000

__Cross-Modal Architecture__

best model at epoch 12

Validation Set Metrics:
- Recall@1: 0.1333
- Recall@5: 0.3629
- Recall@10: 0.4906
- MedianRank: 11.0000

Test Set Metrics:
- Recall@1: 0.1336
- Recall@5: 0.3718
- Recall@10: 0.5033
- MedianRank: 10.0000