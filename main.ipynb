import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns
import math

# load data
DATA_DIR = './tensorflow-datasets/'

# training data = 90% of train set, validation = 10% of train set
# test set predefined on import
# shuffle to prevent ordering
train_ds = tfds.load('fashion_mnist', split='train[:90%]', data_dir=DATA_DIR, shuffle_files=True)
val_ds = tfds.load('fashion_mnist', split='train[-10%:]', data_dir=DATA_DIR, shuffle_files=True)
test_ds = tfds.load('fashion_mnist', split='test', data_dir=DATA_DIR, shuffle_files=True)


# preprocessing function
def preprocess(sample):
    
    # cast to float (will normalize to 0-1 scale in model)
    image = tf.cast(sample['image'], tf.float32)
    
     # one-hot encode labels for use of softmax/probabilty distribution
    label = tf.one_hot(sample['label'], depth=10)
    
    return image, label

# 32 standard batch size
BATCH_SIZE = 32

# apply preprocessing + batching
# shuffle training set to eliminate accidental ordering in batches
train_ds = train_ds.map(preprocess).shuffle(10000).batch(BATCH_SIZE)
val_ds = val_ds.map(preprocess).batch(BATCH_SIZE)
test_ds = test_ds.map(preprocess).batch(BATCH_SIZE)


# general build model function
def build_model(layers=[128], learning_rate=0.001, use_regularizer=False):
    model = tf.keras.Sequential()
    
    # normalize to 0-1 scale
    model.add(tf.keras.layers.Rescaling(1./255, input_shape=(28,28,1)))
    
    # flatten vector
    model.add(tf.keras.layers.Flatten())

    # regularize with L2 (penalize excessively large weights, but keep all features)
    if use_regularizer:
        reg = tf.keras.regularizers.l2(0.001)
    else:
        reg = None
    
    # add hidden layers
    for layer in layers:
        model.add(tf.keras.layers.Dense(layer, activation="relu", kernel_regularizer=reg))
    
    # softmax output layer
    model.add(tf.keras.layers.Dense(10, activation="softmax"))

    # compile model with Adam
    # categorical crossentropy used with one-hot encoded labels
    # track accuracy
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss="categorical_crossentropy",
        metrics=["accuracy"]
    )
    
    return model


# train and evaluate
def train_and_evaluate(layers, lr, use_reg, model_name):
    
    print(f"\n***** Training {model_name} *****")
    
    # compile model with given architecture
    model = build_model(layers, learning_rate=lr, use_regularizer=use_reg)
    
    # early stopping
    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

    # train model
    # also collect loss/ accuracy cuvres
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=20,
        verbose=2,
        callbacks=[early_stop]
    )
    
    val_loss, val_acc = model.evaluate(val_ds, verbose=0)
    print(f"{run_name} Validation Accuracy: {val_acc:.4f}")
    
    test_loss, test_acc = model.evaluate(test_ds, verbose=0)
    print(f"{run_name} Test Accuracy: {test_acc:.4f}")
    
    # confusion matrices
    y_pred_probs = model.predict(test_ds)
    y_pred = np.argmax(y_pred_probs, axis=1)
    y_true = np.concatenate([np.argmax(y.numpy(), axis=1) for x, y in test_ds], axis=0)
 
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8,6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix: {run_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()
    
    # confidence intervals
    n_test = len(y_true)
    p_error = 1 - test_acc
    # caculates 95% confidence interval
    ci = 1.96 * math.sqrt(p_error * (1 - p_error) / n_test)
    print(f"Generalization error: {p_error:.3f} Â± {ci:.3f} (95% CI)")
    
    return model, history


# hidden layers
architectures = [[128], [256, 128]]

# learning rates
learning_rates = [0.001, 0.0005] 

# regularizer?
regularizers = [False, True]

# store model objects
results = {}

# run 8 total experiments
for arch in architectures:
    for lr in learning_rates:
        for reg in regularizers:
            
            run_name = f"Architecture: {arch}, Learning rate: {lr}, Regularize: {reg}"
            model, history = train_and_evaluate(arch, lr, reg, run_name)
            results[run_name] = model

# evaluate on test set
print("\n***** Test Set Evaluation *****")
for run_name, model in results.items():
    test_loss, test_acc = model.evaluate(test_ds, verbose=0)
    print(f"{run_name} Test Accuracy: {test_acc:.4f}")
